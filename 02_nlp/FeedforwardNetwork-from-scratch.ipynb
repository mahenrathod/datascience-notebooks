{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15adaacf",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network from scratch for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb4be5",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. Goal\n",
    "2. Approach\n",
    "3. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfc91e",
   "metadata": {},
   "source": [
    "### 1. Goal\n",
    "\n",
    "- The goal of this notebook is to develop a Feedforward Neural Netowork from scratch for text classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41bb332",
   "metadata": {},
   "source": [
    "### 2. Approach\n",
    "\n",
    "- Transform raw text data into input vectors\n",
    "\n",
    "\n",
    "- Implement Feedforward network with\n",
    "    - Input: Embedding weight matrix\n",
    "    - Interim: Hidden layer(S)\n",
    "    - Output: softmax\n",
    "    \n",
    "    \n",
    "- Implement Stochastic Gradient Descent (SGD) algorithm with\n",
    "    - Forward pass to compute intermediate outputs\n",
    "    - Backward pass (Backpropagation) to compute gradients and update weights of the network\n",
    "    - Dropout for regularisation\n",
    "    - Perform hyperparater tuning\n",
    "        - dropout rate\n",
    "        - embedding size\n",
    "        - Learning rate\n",
    "    - Plot the learning process\n",
    "        - training loss\n",
    "        - validation loss\n",
    "        \n",
    "        \n",
    "- Implement pre-trained word embeddings (GloVe)\n",
    "    - Reinitialise network weights with pre-trained word embeddings from GloVe\n",
    "    - Don't update weight during training (weight freezing, NO backpropagation)\n",
    "    - Perform hyperparameter tuning\n",
    "    - Plot the learning process\n",
    "    \n",
    "    \n",
    "- Implement More hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ade1b4",
   "metadata": {},
   "source": [
    "### 3. Dataset\n",
    "\n",
    "#### AG News Corpus\n",
    "- The data is taken from [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html).\n",
    "    - Created folder structure:\n",
    "        - `data/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "        - `data/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "        - `data/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "#### GloVe Embeddings\n",
    "\n",
    "- The pre-trained GloVe embeddings trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) is available from [here](http://nlp.stanford.edu/data/glove.840B.300d.zip).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f378f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important important python libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680754e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed():\n",
    "    random.seed(123)\n",
    "    np.random.seed(123)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b776e3",
   "metadata": {},
   "source": [
    "### Transform raw text data into input vectors\n",
    "\n",
    "- Transform the raw text into train, dev, and test sets\n",
    "- Since the dataset done't have headers, we will first create custome headers and then load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2867f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a customer Header\n",
    "news_header = [\"label\", \"text\"]\n",
    "\n",
    "# Load news data\n",
    "news_train = pd.read_csv('./data/train.csv', names=news_header)\n",
    "news_dev = pd.read_csv('./data/dev.csv', names=news_header)\n",
    "news_test = pd.read_csv('./data/test.csv', names=news_header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c155f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Venezuelans turned out early\\and in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - South Korean police used water canno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Thousands of Palestinian\\prisoners i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Sporadic gunfire and shelling took place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AP - Dozens of Rwandan soldiers flew into Suda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Reuters - Venezuelans turned out early\\and in ...\n",
       "1      1  Reuters - South Korean police used water canno...\n",
       "2      1  Reuters - Thousands of Palestinian\\prisoners i...\n",
       "3      1  AFP - Sporadic gunfire and shelling took place...\n",
       "4      1  AP - Dozens of Rwandan soldiers flew into Suda..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data\n",
    "news_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0af3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BAGHDAD, Iraq - An Islamic militant group that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Parts of Los Angeles international airport are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Facing a issue that once tripped up his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The leader of militant Lebanese group Hezbolla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>JAKARTA : ASEAN finance ministers ended a meet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  BAGHDAD, Iraq - An Islamic militant group that...\n",
       "1      1  Parts of Los Angeles international airport are...\n",
       "2      1  AFP - Facing a issue that once tripped up his ...\n",
       "3      1  The leader of militant Lebanese group Hezbolla...\n",
       "4      1  JAKARTA : ASEAN finance ministers ended a meet..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dev.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c9b755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Canadian Press - VANCOUVER (CP) - The sister o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AP - The man who claims Gov. James E. McGreeve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>NAJAF, Iraq - Explosions and gunfire rattled t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>LOURDES, France - A frail Pope John Paul II, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Supporters and rivals warn of possible fraud; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Canadian Press - VANCOUVER (CP) - The sister o...\n",
       "1      1  AP - The man who claims Gov. James E. McGreeve...\n",
       "2      1  NAJAF, Iraq - Explosions and gunfire rattled t...\n",
       "3      1  LOURDES, France - A frail Pope John Paul II, b...\n",
       "4      1  Supporters and rivals warn of possible fraud; ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae647c8",
   "metadata": {},
   "source": [
    "Separate the features and lables lists for train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5156c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_Xtrain = news_train['text'].tolist()\n",
    "news_Xdev = news_dev['text'].tolist()\n",
    "news_Xtest = news_test['text'].tolist()\n",
    "\n",
    "news_ytrain = news_train['label'].to_numpy()\n",
    "news_ydev = news_dev['label'].to_numpy()\n",
    "news_ytest = news_test['label'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02bd3ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "2400\n",
      "150\n",
      "150\n",
      "900\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the new datasets\n",
    "print(len(news_Xtrain))\n",
    "print(len(news_ytrain))\n",
    "print(len(news_Xdev))\n",
    "print(len(news_ydev))\n",
    "print(len(news_Xtest))\n",
    "print(len(news_ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca5538a",
   "metadata": {},
   "source": [
    "### Implement Feedforward network\n",
    "\n",
    "**Create input representations**\n",
    "- To build a Feedforward neural network, the input data representation needs to be obtained in two ways; from vocabulary or from one-hot encoding. Since, one-hot encoding reqires large memory capacity, we ill repreent the input documents as a list of vocabulary indices where each wrod corresponds to a vocabulary index.\n",
    "\n",
    "**Text pre-processing pipeline**\n",
    "- To obtrain the vocabulary of words, we will create text pre-processing pipeline as below:\n",
    "    - Tokenise all texts into a list of unigrams\n",
    "    - Remove stop words\n",
    "    - Remove unigrams appearing in less than K documents\n",
    "    - Use the remaining unigrams to create the vocabulary of top-N most frequent unigrams in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29cd8224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the stop words list\n",
    "stop_words = ['a', 'i', \"i'm\", 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', \n",
    "              'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', \n",
    "              'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \n",
    "              'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "              'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', \n",
    "              'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "              'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', \n",
    "              'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "              'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', \n",
    "              'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n",
    "              'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', \n",
    "              'each', 'few', 'more', 'most', 'other', 'some', 'such', \n",
    "              'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', \n",
    "              'just', 'should', \"should've\", 'now', 'll', 're', \n",
    "              've', 'ma', \".\", \",\", \"would\", \"could\", \"must\", \"shall\", \"may\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427156b",
   "metadata": {},
   "source": [
    "### Unigram extraction from a document\n",
    "\n",
    "\n",
    "Implement ngram extraction function `extract_ngrams` where \n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams being extracted (e.g. (1,2) denotes extracting unigrams and bigrams)\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that the data is already tokenised so we will use white space tokenisation\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features\n",
    "    \n",
    "and return \n",
    "- list of all extracted features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ee1366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement ngram extraction\n",
    "def extract_ngrams(x_raw, ngram_range=(1,3), \n",
    "                  token_pattern=f'\\b[A-Za-z][A-Za-z]+\\b',\n",
    "                 stop_words=[], vocab=set()):\n",
    "    token_regex = re.compile(token_pattern)\n",
    "    \n",
    "    # Extract all unigrams by tokenising\n",
    "    x_unigram = [w for w in token_regex.findall(str(x_raw).lower(),) if w not in stop_words]\n",
    "    \n",
    "    # Store the unigrams to be returned\n",
    "    x = []\n",
    "    if ngram_range[0] == 1:\n",
    "        x = x_unigram\n",
    "    \n",
    "    # Generate n-grams from avaialbe unigrams\n",
    "    ngrams = []\n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "        if n==1: \n",
    "            continue\n",
    "        \n",
    "        # Pass a list of lists as an argument for zip\n",
    "        arg_list = [x_unigram]+[x_unigram[i:] for i in range(1, n)]\n",
    "        \n",
    "        # extract tuples of n-grams using zip\n",
    "        # for bigram this should look: list(zip(x_uni, x_uni[1:]))\n",
    "        # align each item x[i] in x_uni with the next one x[i+1]. \n",
    "        # Note that x_uni and x_uni[1:] have different lenghts\n",
    "        # but zip ignores redundant elements at the end of the second list\n",
    "        # Alternatively, this could be done with for loops\n",
    "        x_ngram = list(zip(*arg_list))\n",
    "        ngrams.append(x_ngram)\n",
    "    \n",
    "    for n in ngrams:\n",
    "        for t in n:\n",
    "            x.append(t)\n",
    "            \n",
    "    if len(vocab)>0:\n",
    "        x=[w for w in x if w in vocab]\n",
    "    \n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc886cdf",
   "metadata": {},
   "source": [
    "### Create a vocabular of n-grams\n",
    "\n",
    "Implement `get_vocab` function, where\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams being extracted (e.g. (1,2) denotes extracting unigrams and bigrams)\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency\n",
    "- `keep_topN`: keep top-N more frequent ngrams\n",
    "\n",
    "and return\n",
    "- `vocab`: a set of n-grams that will be used as features\n",
    "- `df`: a document frequency dictionay that contains ngrams as key and their corresponding document frequenciy as values\n",
    "- `ngram_counts`: counts of each ngram in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e3d11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3),\n",
    "             token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',\n",
    "             min_df=0, keep_topN=0, stop_words=[]):\n",
    "\n",
    "    token_regex = re.compile(token_pattern)\n",
    "    df = Counter()\n",
    "    ngram_counts = Counter()\n",
    "    vocab = set()\n",
    "    \n",
    "    # Iterate through each raw text\n",
    "    for x in X_raw:\n",
    "        x_ngram = extract_ngrams(x, ngram_range=ngram_range, \n",
    "                                 token_pattern=token_pattern, \n",
    "                                 stop_words=stop_words)\n",
    "        # Update document frequency and ngram count\n",
    "        df.update(list(set(x_ngram)))\n",
    "        ngram_counts.update(x_ngram)\n",
    "        \n",
    "    # Obtain vocabulary as a set with document frequency > min document frequency\n",
    "    vocab = set([w for w in df if df[w]>=min_df])\n",
    "    \n",
    "    # Keep the top N most frequent vocab\n",
    "    if keep_topN:\n",
    "        vocab =set([w[0] for w in ngram_counts.most_common(keep_topN)\n",
    "                   if w[0] in vocab])\n",
    "        \n",
    "    return vocab, df, ngram_counts\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce628e6a",
   "metadata": {},
   "source": [
    "### Get the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a019594c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news vocab size: 2000\n",
      "['congolese', 'face', 'mobile', 'ibm', 'warning', 'tools', 'gained', 'anticipated', 'refugee', 'kostas', 'port', 'fall', 'friends', 'looking', 'remote', 'right', 'activists', 'wild', 'villa', 'mart', 'jamaica', 'weekly', 'improvements', 'prosecutors', 'bowl', 'resolve', 'row', 'presidential', 'fear', 'quarter', 'believe', 'mistakes', 'general', 'profit', 'got', 'williams', 'problems', 'rain', 'whether', 'pull', 'northern', 'debut', 'michael', 'vijay', 'olympia', 'secretary', 'assault', 'google', 'inventory', 'beating', 'abroad', 'teams', 'america', 'scheduled', 'looks', 'rather', 'sad', 'heard', 'output', 'former', 'program', 'insurer', 'lourdes', 'operator', 'hospitals', 'company', 'historic', 'calling', 'black', 'singh', 'baseman', 'katerina', 'inflationary', 'fuel', 'crowd', 'prince', 'candidate', 'bell', 'cleric', 'affair', 'safety', 'illinois', 'cowboys', 'let', 'indian', 'scotland', 'separate', 'kid', 'injured', 'lines', 'services', 'need', 'pitched', 'east', 'settlements', 'accept', 'us', 'berlusconi', 'lumpur', 'cardinals', 'crashed', 'phone', 'cold', 'prove', 'cautious', 'expects', 'engine', 'nearly', 'available', 'tax', 'played', 'killing', 'knows', 'pentagon', 'boosted', 'claim', 'gary', 'canada', 'returning', 'rico', 'judges', 'adopted', 'annual', 'straight', 'arms', 'become', 'prominent', 'mortgage', 'greece', 'trading', 'dispute', 'sweep', 'population', 'roman', 'reversed', 'appeal', 'tutsi', 'gymnasts', 'borders', 'qaida', 'topping', 'change', 'fullquote', 'process', 'airlines', 'saudi', 'hottest', 'making', 'index', 'kuala', 'firms', 'opec', 'suspended', 'last', 'national', 'korea', 'classic', 'committee', 'financial', 'report', 'gorda', 'climbed', 'draw', 'turned', 'bad', 'damage', 'amid', 'metres', 'concerns', 'practice', 'wisconsin', 'hungarian', 'million', 'ian', 'use', 'man', 'aware', 'bankruptcy', 'set', 'walked', 'race', 'self', 'loans', 'van', 'iraqi', 'hamilton', 'yukos', 'power', 'case', 'cp', 'bid', 'moscow', 'ministry', 'life', 'hockey', 'went', 'leading', 'robert', 'tom', 'central', 'economic', 'madrid', 'mary', 'always', 'yasser', 'family', 'term', 'mdt', 'promotion', 'guard', 'expected', 'party', 'going', 'player', 'lower', 'important', 'programme', 'camp', 'buildings', 'czech', 'rising', 'arbitration', 'beginning', 'approved', 'approve', 'brazil', 'professional', 'co', 'commission', 'data', 'diego', 'according', 'criminal', 'child', 'china', 'vault', 'italy', 'murder', 'opened', 'congress', 'equity', 'hamid', 'delivered', 'send', 'suspected', 'wounding', 'uae', 'airways', 'factories', 'draft', 'inc', 'earnings', 'showing', 'november', 'top', 'determine', 'dodgers', 'path', 'check', 'struggling', 'construction', 'violations', 'single', 'seven', 'online', 'fischer', 'accepted', 'andrew', 'job', 'night', 'uprising', 'security', 'raise', 'help', 'rules', 'opening', 'source', 'defense', 'cars', 'highs', 'najaf', 'carter', 'wind', 'jail', 'division', 'sun', 'ag', 'young', 'starts', 'among', 'american', 'talk', 'storm', 'reached', 'water', 'wing', 'ratings', 'auto', 'kmrt', 'hare', 'control', 'gas', 'headed', 'foot', 'par', 'rise', 'coach', 'blow', 'jcp', 'playing', 'minnesota', 'later', 'retailer', 'kosuke', 'hamm', 'drug', 'winner', 'allowed', 'quincy', 'backpacker', 'investors', 'breaking', 'win', 'earned', 'costs', 'ottawa', 'less', 'much', 'bank', 'sport', 'stores', 'fighters', 'ticket', 'estimated', 'independence', 'cup', 'ap', 'effective', 'replaced', 'plans', 'montreal', 'jose', 'fired', 'class', 'tickets', 'paris', 'andy', 'asian', 'embassy', 'moved', 'launch', 'buried', 'caused', 'goal', 'good', 'decline', 'inflation', 'falling', 'disruptions', 'giving', 'begin', 'olympic', 'disaster', 'track', 'improvement', 'knocked', 'antonio', 'suspects', 'selling', 'tim', 'course', 'fans', 'farm', 'angeles', 'weapons', 'rushed', 'little', 'july', 'cease', 'oakland', 'point', 'successful', 'ten', 'filed', 'old', 'managers', 'seattle', 'medal', 'motors', 'hansen', 'sanctions', 'rejected', 'japan', 'brendan', 'inquiry', 'alleged', 'meet', 'evening', 'soldiers', 'bill', 'seeking', 'end', 'extra', 'stock', 'talks', 'soccer', 'golf', 'every', 'makers', 'becomes', 'sent', 'champions', 'wife', 'al', 'offering', 'asia', 'showed', 'dropped', 'gains', 'sold', 'rare', 'sites', 'regular', 'son', 'airport', 'planned', 'jackson', 'eagerly', 'training', 'slumping', 'quickinfo', 'died', 'drew', 'nelson', 'mariel', 'miles', 'venezuela', 'baghdad', 'changes', 'added', 'higher', 'referendum', 'plan', 'thought', 'hundreds', 'debt', 'hospital', 'disappeared', 'americans', 'chinese', 'air', 'separatist', 'democracy', 'required', 'stage', 'minute', 'tiger', 'visit', 'dream', 'kevin', 'den', 'pakistan', 'pinch', 'posted', 'doubled', 'switzerland', 'isn', 'career', 'house', 'increasing', 'sox', 'arrived', 'pick', 'fencing', 'middle', 'operation', 'revealed', 'defensive', 'russian', 'boy', 'bought', 'spitz', 'staff', 'event', 'known', 'look', 'women', 'care', 'proved', 'sydney', 'hunger', 'vote', 'businesses', 'day', 'battles', 'alert', 'ukraine', 'exports', 'wis', 'withhold', 'radioactive', 'private', 'euros', 'meetings', 'french', 'bills', 'high', 'main', 'freestyle', 'beat', 'declare', 'used', 'interim', 'scored', 'preliminaries', 'allen', 'campaign', 'landing', 'britain', 'fifth', 'kenteris', 'work', 'mike', 'show', 'sri', 'build', 'wave', 'not', 'begins', 'swimming', 'highly', 'trees', 'facing', 'took', 'school', 'range', 'political', 'haven', 'driven', 'mass', 'joined', 'appealed', 'closer', 'policy', 'rival', 'players', 'decade', 'convicted', 'revolution', 'away', 'kerry', 'ticker', 'chicago', 'minister', 'lost', 'attempt', 'asked', 'defused', 'along', 'opponents', 'customers', 'street', 'super', 'typhoon', 'wants', 'hours', 'ireland', 'applied', 'published', 'suspension', 'lows', 'suspicious', 'toronto', 'started', 'guillen', 'chavez', 'launched', 'lowest', 'no', 'quot', 'sea', 'place', 'reserve', 'international', 'khartoum', 'boxing', 'twins', 'tomas', 'burundi', 'threat', 'citizens', 'web', 'cleveland', 'verge', 'moves', 'union', 'spokesman', 'helping', 'nine', 'euro', 'bringing', 'pool', 'hilton', 'votes', 'west', 'psychological', 'communist', 'challenge', 'consecutive', 'residents', 'hitting', 'computer', 'september', 'costas', 'extending', 'champion', 'near', 'size', 'tokyo', 'maktoum', 'keller', 'trial', 'competition', 'pieter', 'mason', 'share', 'europe', 'supplies', 'grim', 'ethnic', 'berdych', 'payments', 'johnson', 'overboard', 'rescue', 'steve', 'border', 'far', 'punta', 'fellow', 'since', 'put', 'figures', 'netherlands', 'england', 'current', 'network', 'benefits', 'product', 'confidence', 'police', 'region', 'huge', 'calls', 'sudan', 'teenager', 'twenty', 'senate', 'civil', 'impact', 'championship', 'accounting', 'arlington', 'backed', 'improved', 'immediately', 'zealand', 'slump', 'payment', 'bloomberg', 'atlanta', 'reports', 'questions', 'western', 'conference', 'conditions', 'spot', 'dow', 'investigation', 'microsoft', 'monthly', 'massachusetts', 'hard', 'apparently', 'third', 'opener', 'slam', 'urban', 'energy', 'avoid', 'winged', 'washington', 'investor', 'north', 'expressed', 'ohio', 'strong', 'terrorism', 'order', 'seen', 'withdrew', 'banned', 'knee', 'thumb', 'media', 'title', 'hd', 'part', 'shrugged', 'twice', 'qualifying', 'group', 'germany', 'detroit', 'hoogenband', 'children', 'gold', 'continues', 'ending', 'materials', 'arizona', 'doping', 'ask', 'shot', 'survived', 'white', 'association', 'collect', 'percent', 'http', 'decided', 'singles', 'hands', 'fought', 'iverson', 'palestinians', 'rebound', 'carrier', 'european', 'teixeira', 'game', 'protests', 'sources', 'gathering', 'eight', 'step', 'sec', 'nortel', 'heading', 'return', 'contract', 'statement', 'manager', 'plc', 'three', 'devices', 'invoices', 'violent', 'strike', 'demand', 'jobs', 'accused', 'closely', 'employees', 'leonard', 'organization', 'morning', 'rebounded', 'details', 'prompted', 'toward', 'oq', 'finished', 'officer', 'regulators', 'involving', 'peter', 'taking', 'hour', 'aimed', 'english', 'victory', 'tests', 'massacre', 'paul', 'string', 'commit', 'four', 'agency', 'challenges', 'ancient', 'threatened', 'rolled', 'meters', 'friday', 'catch', 'stand', 'straits', 'june', 'raising', 'adam', 'season', 'tell', 'haas', 'rights', 'eliminated', 'acquisition', 'given', 'potential', 'latham', 'dollar', 'taxes', 'signs', 'eighth', 'estimates', 'opponent', 'also', 'ing', 'href', 'awaited', 'best', 'conflict', 'others', 'crash', 'fourth', 'areas', 'venus', 'australian', 'basketball', 'personal', 'although', 'insurers', 'wednesday', 'thanou', 'winds', 'reforms', 'pressure', 'tearing', 'club', 'expansion', 'states', 'iran', 'eased', 'tourist', 'wal', 'come', 'scandal', 'offset', 'number', 'stunning', 'kitajima', 'cbs', 'crowded', 'delivery', 'lead', 'woods', 'conspiring', 'greek', 'second', 'ease', 'aides', 'homer', 'men', 'raised', 'named', 'back', 'trying', 'edge', 'proposal', 'championships', 'legal', 'paperwork', 'done', 'gov', 'search', 'call', 'rose', 'strip', 'criticism', 'creating', 'bujumbura', 'beijing', 'result', 'crowds', 'boston', 'profile', 'pulled', 'offered', 'prepared', 'delegation', 'georgia', 'village', 'barclays', 'hamas', 'winning', 'support', 'hong', 'democratic', 'tour', 'strength', 'conspiracy', 'peace', 'silver', 'el', 'com', 'offers', 'said', 'london', 'person', 'united', 'laws', 'half', 'bases', 'throw', 'tanks', 'fla', 'preliminary', 'forward', 'funds', 'view', 'home', 'held', 'site', 'operations', 'week', 'picture', 'tied', 'aspx', 'largely', 'telephone', 'worst', 'convention', 'capacity', 'country', 'received', 'closing', 'un', 'intelligence', 'cause', 'grant', 'declared', 'ramallah', 'ninth', 'chairman', 'food', 'charged', 'sprint', 'protest', 'prime', 'injury', 'lay', 'pontiff', 'auction', 'front', 'cents', 'satellite', 'aside', 'maker', 'magazine', 'greeks', 'jean', 'negotiate', 'announced', 'bhp', 'offer', 'two', 'shrine', 'board', 'led', 'increase', 'weekend', 'corruption', 'protesters', 'better', 'level', 'fre', 'market', 'head', 'trap', 'charley', 'cost', 'justin', 'ambassador', 'law', 'popular', 'arab', 'doubts', 'helicopter', 'fire', 'six', 'needed', 'quit', 'negotiations', 'voted', 'whose', 'billiton', 'break', 'www', 'massive', 'innings', 'dead', 'world', 'puerto', 'late', 'radical', 'interest', 'death', 'silvio', 'panel', 'gap', 'technology', 'ariel', 'friendly', 'city', 'waiting', 'bonds', 'africa', 'system', 'crown', 'sadr', 'injuries', 'serious', 'games', 'nyse', 'perhaps', 'finals', 'cent', 'scott', 'pitcher', 'klete', 'upset', 'sixth', 'thursday', 'flood', 'shi', 'fed', 'highest', 'medical', 'capital', 'pittsburgh', 'device', 'mutual', 'executives', 'tutsis', 'gt', 'sprinters', 'produced', 'bgp', 'elbow', 'jersey', 'chancellor', 'recall', 'money', 'try', 'move', 'cities', 'chris', 'fight', 'hot', 'ever', 'phelps', 'billions', 'easy', 'efforts', 'delay', 'groups', 'breaststroke', 'qaeda', 'walk', 'test', 'came', 'battle', 'green', 'getting', 'touted', 'raids', 'post', 'france', 'calif', 'clashes', 'toyota', 'relay', 'local', 'taken', 'say', 'flash', 'usa', 'reportedly', 'drove', 'despite', 'helped', 'growth', 'torri', 'killed', 'fractured', 'malaysia', 'mandate', 'regulatory', 'buy', 'amat', 'rally', 'lawsuit', 'observers', 'wanted', 'individual', 'building', 'firm', 'go', 'woman', 'nepal', 'administration', 'thorpe', 'religious', 'fears', 'paid', 'advanced', 'troubled', 'controversial', 'chip', 'supporters', 'development', 'unit', 'arafat', 'meeting', 'industry', 'find', 'st', 'athletes', 'picked', 'operative', 'finance', 'anti', 'divisions', 'overnight', 'suspend', 'plus', 'linked', 'great', 'canadian', 'tore', 'turkish', 'pre', 'indians', 'claims', 'forecast', 'saw', 'even', 'outlooks', 'island', 'table', 'final', 'signed', 'open', 'cuts', 'collective', 'premier', 'close', 'opposition', 'one', 'caught', 'based', 'captain', 'putter', 'county', 'motor', 'compete', 'kuwait', 'militants', 'quarterly', 'powder', 'defeat', 'possible', 'well', 'press', 'became', 'militia', 'philippines', 'lowe', 'flu', 'defence', 'research', 'owners', 'orioles', 'deficit', 'companies', 'effort', 'economy', 'houston', 'sale', 'motorola', 'saturday', 'complete', 'following', 'lawyers', 'homered', 'rebels', 'days', 'legg', 'parade', 'miss', 'kathmandu', 'associated', 'southwest', 'august', 'monday', 'lives', 'investment', 'sudanese', 'ipo', 'leaders', 'shiite', 'join', 'inning', 'amateur', 'holding', 'custody', 'israel', 'flew', 'fund', 'heat', 'army', 'giant', 'las', 'protect', 'relief', 'banks', 'period', 'outlook', 'federal', 'broke', 'breakaway', 'stocks', 'cash', 'sex', 'created', 'cross', 'tomorrow', 'release', 'didn', 'summer', 'living', 'slower', 'hall', 'next', 'rebel', 'yet', 'darfur', 'david', 'wreckage', 'moqtada', 'response', 'rod', 'eastern', 'surgery', 'business', 'events', 'islamabad', 'congestion', 'roddick', 'workers', 'jump', 'selection', 'car', 'medallist', 'bay', 'shoulder', 'threatening', 'earn', 'san', 'tech', 'falconio', 'continued', 'ossetia', 'families', 'singapore', 'millions', 'sentiment', 'pile', 'victims', 'florida', 'texas', 'revenue', 'idea', 'cincinnati', 'version', 'romania', 'vice', 'recent', 'swimmer', 'called', 'poor', 'giants', 'tuesday', 'quickly', 'philadelphia', 'labor', 'suffered', 'crude', 'semifinals', 'charges', 'holiday', 'corp', 'athletics', 'concerned', 'rates', 'together', 'liverpool', 'hearing', 'sense', 'hope', 'italian', 'body', 'fighting', 'ex', 'republic', 'internet', 'senior', 'janus', 'crisis', 'shares', 'auditors', 'prosecutor', 'barrel', 'long', 'rwandan', 'afghan', 'commodities', 'agreed', 'kabul', 'keep', 'another', 'powerful', 'issues', 'registration', 'afternoon', 'average', 'caracas', 'facilities', 'rathore', 'colorado', 'soon', 'louis', 'weeks', 'prison', 'surge', 'runs', 'leftist', 'reported', 'attacks', 'arena', 'tennis', 'concern', 'dug', 'fueled', 'pass', 'royal', 'five', 'clemens', 'dimarco', 'previous', 'missed', 'action', 'jones', 'appeared', 'missing', 'year', 'ended', 'profits', 'leave', 'major', 'military', 'reduced', 'struck', 'heavy', 'venezuelan', 'television', 'trade', 'outside', 'war', 'early', 'believes', 'midday', 'improve', 'nfl', 'jury', 'luxury', 'recovering', 'chess', 'berlin', 'desperate', 'renewed', 'want', 'militant', 'dawn', 'detained', 'deal', 'electoral', 'hurt', 'analysts', 'aug', 'mariners', 'big', 'witnesses', 'networks', 'recently', 'plotting', 'town', 'tight', 'ways', 'gaza', 'might', 'continue', 'heart', 'troops', 'says', 'announcement', 'football', 'chain', 'sharp', 'ahead', 'force', 'exhibition', 'likely', 'disciplinary', 'denied', 'session', 'baltimore', 'mark', 'global', 'bryant', 'largest', 'south', 'full', 'delegates', 'almost', 'court', 'terrorists', 'radio', 'earlier', 'surging', 'california', 'hold', 'operating', 'judge', 'human', 'faster', 'southern', 'cut', 'vs', 'presidency', 'probably', 'first', 'minutes', 'russia', 'industrial', 'park', 'faces', 'markets', 'mining', 'voters', 'years', 'electricity', 'happen', 'hitter', 'yard', 'homes', 'total', 'production', 'least', 'alexander', 'official', 'gave', 'foreign', 'within', 'comes', 'government', 'gymnastics', 'loyal', 'dallas', 'ailing', 'jumped', 'authority', 'billing', 'venezuelans', 'battled', 'british', 'restaurants', 'envoy', 'trouble', 'struggled', 'amp', 'run', 'sell', 'summit', 'without', 'consumer', 'ltd', 'lt', 'ii', 'loss', 'herat', 'recovery', 'nigeria', 'leader', 'governor', 'levels', 'stake', 'increased', 'growing', 'start', 'band', 'products', 'hellip', 'consider', 'olympics', 'advance', 'yesterday', 'baseball', 'closed', 'decision', 'several', 'buyers', 'rookie', 'saying', 'boys', 'medals', 'muslim', 'university', 'gasoline', 'slip', 'oust', 'real', 'citing', 'starting', 'new', 'claiming', 'pga', 'delayed', 'target', 'embarrassing', 'jerusalem', 'settlement', 'losing', 'brian', 'information', 'issued', 'streak', 'prices', 'whistling', 'oldest', 'told', 'butterfly', 'faced', 'dismissed', 'african', 'torture', 'rule', 'james', 'girl', 'weaker', 'diplomatic', 'stay', 'play', 'refused', 'exchange', 'began', 'reform', 'already', 'francisco', 'newspaper', 'emergency', 'card', 'refugees', 'either', 'retail', 'match', 'holiest', 'large', 'joining', 'nestle', 'iraq', 'hal', 'met', 'attempts', 'running', 'chief', 'outfielder', 'reach', 'jason', 'won', 'gone', 'low', 'austria', 'countries', 'like', 'reds', 'service', 'nations', 'nasdaq', 'mcdonald', 'situation', 'tournament', 'regional', 'showdown', 'area', 'bargain', 'stadium', 'activity', 'surged', 'sharply', 'nation', 'working', 'member', 'lift', 'georgian', 'kmart', 'thing', 'month', 'inevitable', 'boost', 'kong', 'march', 'violation', 'authorities', 'claimed', 'xinhuanet', 'landmark', 'host', 'worries', 'york', 'manage', 'signing', 'thousands', 'zagunis', 'drive', 'floods', 'sweeping', 'appears', 'warnings', 'maoist', 'rest', 'warned', 'movement', 'goog', 'toll', 'positions', 'though', 'targets', 'terror', 'muqtada', 'hugo', 'team', 'australia', 'tested', 'practices', 'numbers', 'militiamen', 'leg', 'dozen', 'series', 'quest', 'cycling', 'hit', 'soaring', 'counting', 'uk', 'hurricane', 'defending', 'becoming', 'blair', 'urged', 'prisoners', 'round', 'futures', 'pushed', 'swept', 'miracle', 'testing', 'quote', 'prescription', 'parmalat', 'john', 'news', 'time', 'image', 'small', 'exporters', 'supply', 'hopes', 'bj', 'multi', 'rivals', 'hits', 'nikkei', 'ordered', 'ravaged', 'needs', 'billion', 'mat', 'coast', 'served', 'changed', 'really', 'kobe', 'ite', 'elections', 'watched', 'left', 'advertising', 'armed', 'ground', 'maria', 'thanks', 'bring', 'sunday', 'newly', 'jay', 'hostile', 'george', 'pitch', 'bronze', 'soldier', 'losses', 'ny', 'quarterback', 'nuclear', 'drop', 'office', 'pay', 'history', 'korean', 'story', 'cos', 'violence', 'spending', 'truce', 'los', 'league', 'coming', 'light', 'occupied', 'equipment', 'months', 'many', 'securities', 'still', 'clear', 'name', 'officials', 'double', 'tens', 'registered', 'department', 'sales', 'patriots', 'daily', 'net', 'free', 'teammate', 'oil', 'state', 'biggest', 'serving', 'ditch', 'pm', 'grew', 'drugs', 'get', 'road', 'stopped', 'made', 'side', 'wounded', 'german', 'sprinter', 'today', 'terrorist', 'expos', 'depot', 'knew', 'due', 'hotel', 'haiti', 'latest', 'shooting', 'interview', 'explosion', 'fast', 'dozens', 'brought', 'withdraw', 'way', 'tie', 'congo', 'volleyball', 'pope', 'sharon', 'familiar', 'kept', 'finish', 'upbeat', 'october', 'jays', 'pro', 'assembly', 'unrest', 'reduction', 'agent', 'people', 'speed', 'sept', 'upi', 'beaten', 'stewart', 'something', 'shia', 'bargaining', 'performance', 'sports', 'akron', 'released', 'programs', 'applications', 'snow', 'invest', 'slow', 'phillies', 'positive', 'fleet', 'captured', 'college', 'times', 'see', 'ban', 'line', 'dollars', 'howard', 'tony', 'around', 'corporate', 'chance', 'sign', 'ryder', 'presence', 'remain', 'results', 'points', 'assistant', 'tumbled', 'initial', 'blue', 'costco', 'grand', 'center', 'slashed', 'seed', 'election', 'india', 'members', 'moderate', 'blockade', 'ago', 'settler', 'gerhard', 'public', 'sick', 'focus', 'per', 'wholesale', 'kingdom', 'base', 'rallied', 'past', 'gain', 'insurance', 'brokerage', 'charge', 'enter', 'president', 'jr', 'strained', 'disarm', 'globe', 'watch', 'vegas', 'sending', 'palestinian', 'bomb', 'declined', 'reuters', 'afghanistan', 'halliburton', 'price', 'bush', 'wall', 'management', 'stop', 'short', 'freddie', 'executive', 'leaving', 'take', 'things', 'housing', 'hand', 'federer', 'style', 'fresh', 'forces', 'lose', 'make', 'matter', 'preseason', 'yankees', 'expectations', 'agreement', 'special', 'meter', 'de', 'give', 'bit', 'israeli', 'jewish', 'survey', 'approval', 'arrested', 'islamic', 'bids', 'dell', 'including', 'wide', 'judo', 'athens', 'edwards', 'diving', 'refinancings', 'red', 'kansas', 'block', 'lack', 'rangers', 'fell', 'record', 'store', 'star', 'calm', 'bobby', 'relating', 'field', 'instead', 'evidence', 'attack', 'branch', 'finally', 'hoping', 'failed', 'roger', 'key', 'credit', 'province', 'father', 'merger', 'behind', 'ranked', 'setting', 'question', 'republican', 'easing', 'portugal', 'income', 'enough', 'japanese', 'found', 'northeast', 'schroeder', 'airline', 'future', 'health', 'afp', 'arch', 'brown', 'torn', 'software', 'karzai', 'playoff', 'tenders', 'mac', 'stayed', 'watson', 'rate', 'across', 'commerce', 'wake', 'treasury', 'promised', 'sardinia', 'larry', 'holy', 'fraud', 'teammates']\n"
     ]
    }
   ],
   "source": [
    "keep_topN = 2000\n",
    "ngram_range = (1,1)\n",
    "min_df = 0\n",
    "(vocab_set,\n",
    "df,\n",
    "ngram_counts) = get_vocab(news_Xtrain, ngram_range=ngram_range,\n",
    "                          min_df=min_df, keep_topN=keep_topN,\n",
    "                          stop_words=stop_words)\n",
    "news_vocab = list(vocab_set)\n",
    "print(f'news vocab size: {len(news_vocab)}')\n",
    "print(news_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850a99c",
   "metadata": {},
   "source": [
    "As far as vocabulary size is concerned, the larger vocabulary gives larger breadth of words to be picked up during training. However, for this experiment, I have restricted to size 2000. In the upcoming section, we will do hyperparameter tuning during training to get the reasonable accuracy while using this vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e94dbb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word to id dictionary for future use\n",
    "word2id = dict(zip(news_vocab, range(len(news_vocab))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1552d",
   "metadata": {},
   "source": [
    "### Convert the list of unigrams  into a list of vocabulary indices\n",
    "- Storing actual one-hot vectors into memory for all words in the entire data set is prohibitive. Instead, we will store word indices in the vocabulary and look-up the weight matrix. This is equivalent of doing a dot product between an one-hot vector and the weight matrix. \n",
    "\n",
    "- Implement a function `extract_ngrams_for_train_dev_test_sets` to represent documents in train, dev and test sets as lists of words in the vocabulary, where\n",
    "    - `ngram_range`: a tuple of two integers denoting the type of ngrams being extracted (e.g. (1,2) denotes extracting unigrams and bigrams)\n",
    "    - `vocab`: a vocabulary set\n",
    "- and returns\n",
    "    - `Xtrain_ngrams`: ngrams for train set\n",
    "    - `Xdev_ngrams`: ngrams for dev set\n",
    "    - `Xtest_ngrams`: ngrams for test set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8017b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams_for_train_dev_test_sets(ngram_range, vocab):\n",
    "    Xtrain = news_Xtrain\n",
    "    Xdev   = news_Xdev\n",
    "    Xtest  = news_Xtest\n",
    "    \n",
    "    # Extract n-grams for training set\n",
    "    Xtrain_ngrams = list()\n",
    "    for i in tnrange(len(Xtrain)):\n",
    "        Xtrain_ngrams.append(extract_ngrams(Xtrain[i], \n",
    "                                            ngram_range=ngram_range, token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',\n",
    "                                            stop_words=stop_words, vocab=vocab))\n",
    "    # Extract n-grams for development set\n",
    "    Xdev_ngrams = list()\n",
    "    for i in tnrange(len(Xdev)):\n",
    "        Xdev_ngrams.append(extract_ngrams(Xdev[i], \n",
    "                                            ngram_range=ngram_range, token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',\n",
    "                                            stop_words=stop_words, vocab=vocab))\n",
    "    # Extract n-grams for test set\n",
    "    Xtest_ngrams = list()\n",
    "    for i in tnrange(len(Xtest)):\n",
    "        Xtest_ngrams.append(extract_ngrams(Xtest[i], \n",
    "                                            ngram_range=ngram_range, token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',\n",
    "                                            stop_words=stop_words, vocab=vocab))\n",
    "        \n",
    "    return Xtrain_ngrams, Xdev_ngrams, Xtest_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "068b39f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bbc0c13339474493a22a72b7b58b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fde69e01a84197ab33a85dfa1e8e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0c17af75d342c6a43cf0dc54e2f4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(news_Xtrain_ngrams, \n",
    " news_Xdev_ngrams, \n",
    " news_Xtest_ngrams) = extract_ngrams_for_train_dev_test_sets(ngram_range, news_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b73a772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into list of indices\n",
    "news_Xtrain_ngrams_indices = []\n",
    "for ngrams in news_Xtrain_ngrams:\n",
    "    news_Xtrain_ngrams_indices.append([word2id.get(ngram, -1) for ngram in ngrams])\n",
    "    \n",
    "news_Xdev_ngrams_indices = []\n",
    "for ngrams in news_Xdev_ngrams:\n",
    "    news_Xdev_ngrams_indices.append([word2id.get(ngram, -1) for ngram in ngrams])\n",
    "\n",
    "news_Xtest_ngrams_indices = []\n",
    "for ngrams in news_Xtest_ngrams:\n",
    "    news_Xtest_ngrams_indices.append([word2id.get(ngram, -1) for ngram in ngrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d39fdea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reuters', 'venezuelans', 'turned', 'early', 'large', 'numbers', 'sunday', 'vote', 'historic', 'referendum', 'either', 'left', 'wing', 'president', 'hugo', 'chavez', 'office', 'give', 'new', 'mandate', 'next', 'two', 'years']\n",
      "[1889, 1483, 163, 1379, 1577, 1655, 1714, 491, 66, 441, 1573, 1707, 295, 1878, 1650, 575, 1727, 1919, 1534, 1096, 1243, 965, 1460]\n"
     ]
    }
   ],
   "source": [
    "# check the sample of ngram and corresponging vocabulary index\n",
    "print(news_Xtrain_ngrams[0])\n",
    "print(news_Xtrain_ngrams_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eec2d2",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network Architecture\n",
    "\n",
    "The steps to create Feedforward neural network as as following:\n",
    "\n",
    "- Pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n",
    "\n",
    "where $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n",
    "\n",
    "- Then $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n",
    "\n",
    "$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n",
    "\n",
    "- Finally the hidden layer is passed to the output layer:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W) $$ \n",
    "where $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n",
    "\n",
    "- During training, $\\mathbf{a}_1$ is going to be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n",
    "\n",
    "- The network will also be extended to a deeper architecture by passing a hidden layer to another one:\n",
    "\n",
    "$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i $$\n",
    "\n",
    "$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b33517",
   "metadata": {},
   "source": [
    "Create a new function called `get_weight_matrix` which will be used to initialise weight matrix for Feedforward nueral network. The function takes as input:\n",
    "\n",
    "- `row_size`: The row size of the weight matrix\n",
    "- `col_size`: The column size of the weight matrix\n",
    "- `init_val`: The initialisation values that are used to create a range of values from negative to positive\n",
    "\n",
    "and returns:\n",
    "- `init_W`: The weight matrix initialised with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a4b74d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_matrix(row_size, col_size, init_val):\n",
    "    init_W = np.zeros((row_size, col_size))\n",
    "    for j in range(row_size):\n",
    "        weights = np.array(np.random.uniform(-1 * init_val, init_val, col_size)).astype(np.float32)\n",
    "        init_W[j] = weights\n",
    "    return init_W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6f79e",
   "metadata": {},
   "source": [
    "### Network Training\n",
    "\n",
    "First we need to define the parameters of our network by initialising the weight matrix. To do this, we will create a function `network_weights` that takes inputs:\n",
    "\n",
    "- `vocab_size`: The size of the vocabulary\n",
    "- `embedding_dim`: The size of the word embeddings\n",
    "- `hidden_dim`: The list of the sizes of hidden layers. Empty if there are not hidden layers between the average embedding and the output layer\n",
    "- `num_classes`: The number of classes for the output layer\n",
    "\n",
    "and returns:\n",
    "- `W`: A dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers\n",
    "\n",
    "Note: We have to make sure that the dimensionality of each weight matrix is compatible with the previous and next weight matrix, otherwise the forward and backward pass won't be able to perform. We will also consider using np.float32 precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7568e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_weights(vocab_size=1000, embedding_dim=300,\n",
    "                   hidden_dim=[], num_classes=3, init_val=0.5):\n",
    "    \n",
    "    # initialise weights\n",
    "    W = []\n",
    "    \n",
    "    # input layer weight matrix for word embeddings\n",
    "    W.append({\n",
    "        \"embedding_matrix\": get_weight_matrix(vocab_size, embedding_dim, init_val)\n",
    "    })\n",
    "    \n",
    "    # hidden layer weight matrix\n",
    "    row_size = embedding_dim\n",
    "    for j in range(len(hidden_dim)):\n",
    "        col_size = hidden_dim[j]\n",
    "        W.append({\"weights\": get_weight_matrix(row_size, col_size, init_val)})\n",
    "        row_size = col_size\n",
    "        \n",
    "    # output layer weight matrix\n",
    "    col_size = num_classes\n",
    "    W.append({\"weights\": get_weight_matrix(row_size, col_size, init_val)})\n",
    "    \n",
    "    return W\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c5e3fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'embedding_matrix': array([[ 0.19631127, -0.05967212, -0.06178562,  0.2650961 ],\n",
      "       [ 0.065642  , -0.41509584,  0.08267109,  0.31484371],\n",
      "       [-0.16293362,  0.42757657,  0.25071701,  0.07406382]])}, {'weights': array([[ 0.25164399, -0.42085105],\n",
      "       [ 0.35938907,  0.32150412],\n",
      "       [ 0.40987167, -0.3713688 ],\n",
      "       [-0.41821992, -0.36158442]])}, {'weights': array([[-0.10062129, -0.07569314],\n",
      "       [ 0.06221838, -0.37775645]])}]\n"
     ]
    }
   ],
   "source": [
    "# sanity check to see the network weight generation and their dimensions\n",
    "print(network_weights(vocab_size=3, embedding_dim=4, hidden_dim=[2], num_classes=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eca034",
   "metadata": {},
   "source": [
    "### softmax function\n",
    "\n",
    "Due to floating point limitations in numPy, softmax frequently gets `NaN` and `inf` error. To eliminate this error, a softmax normalisation technique is used by subtracting the maximum value of z from the given z value. This way, softmax is normalised across all values for every forward pass and will not generate the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc7d9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z = z - np.max(z)\n",
    "    return np.exp(z) / np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e0002b",
   "metadata": {},
   "source": [
    "### Categorical Loss function\n",
    "\n",
    "The categorical loss function is used to calculate the objective loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad2544f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_loss(X, Y, W):\n",
    "    lossse = list()\n",
    "    num_classes = 3\n",
    "    for i, x in enumerate(X):\n",
    "        out_vals = forward_pass(x, W, dropout_rate=0.0)\n",
    "        \n",
    "        # one-hot vector to represent the correct class during categorical loss calculation\n",
    "        y = one_hot_vector(Y[i]-1, num_classes)\n",
    "        loss = -y * np.log(out_vals['y_pred'].T)\n",
    "        losses.append(np.sum(loss.T))\n",
    "    return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66ac1d",
   "metadata": {},
   "source": [
    "### Categorical Loss Derivative function\n",
    "\n",
    "We need to also create the derivative function to calculate the derivative of categorical loss during backward pass (aka. backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57bd1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_loss_derivative(y, y_pred):\n",
    "    loss_der = y_preds.T - y\n",
    "    return loss_der.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53134fac",
   "metadata": {},
   "source": [
    "### ReLU and ReLU Derivative function\n",
    "\n",
    "We will not implement `relu` function to introduce non-liniearity afte hidden layers. This function will at as activation function during forward pass.\n",
    "\n",
    "$$relu(z_i)= max(z_i,0)$$\n",
    "\n",
    "And the `relu_derivative` function is used to compute its derivative during backward pass (aka. backpropagation)\n",
    "\n",
    "  $$relu\\_derivative(z_i)=0$$ if $z_i$<=0, 1 otherwise.\n",
    "  \n",
    "Note: Both function take as input a vector $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a0e8df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    dz = np.copy(z)\n",
    "    dz[np.where(dz>0)] = 1\n",
    "    dz[np.where(dz<=0)] = 0\n",
    "    return dzå"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcea736",
   "metadata": {},
   "source": [
    "A derivative of ReLU is differentiable at all points except at 0. The left derivative at z<0 is 0 and right derivative at z>0 is 1. But derivative at z=0 is technically undefined. In Colloquial terms, the function is not smooth at z=0 and there are many possible lines (slops) we could fit through. So that do we do here? Basically, we can arbitrarily choose a value as 0, 0.5, or 1 but for simplicity we can choose 0 for z=0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4d2c2",
   "metadata": {},
   "source": [
    "### Dropout mask\n",
    "\n",
    "During training, we will apply dropout mask element-wise to do model finetuning to avoid overfitting. This masking will be done after the ReLU activation function (i.e. vector of ones with random percentage set to zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a8a8d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(size, dropout_rate):\n",
    "    dropout_vec = np.ones((size))\n",
    "    idx = np.random.choice(len(dropout_vec), \n",
    "                          size=int(dropout_rate * size), \n",
    "                          replace=False)\n",
    "    dropout_vec[idx] = 0\n",
    "    return dropout_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d16f5fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 0. 0. 1.]\n",
      "[1. 1. 0. 1. 0. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(dropout_mask(10, 0.2))\n",
    "print(dropout_mask(10, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72116bf",
   "metadata": {},
   "source": [
    "Work in progress..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07483a11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
