{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15adaacf",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network from scratch for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfb4be5",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. Goal\n",
    "2. Approach\n",
    "3. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfc91e",
   "metadata": {},
   "source": [
    "### 1. Goal\n",
    "\n",
    "- The goal of this notebook is to develop a Feedforward Neural Netowork from scratch for text classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41bb332",
   "metadata": {},
   "source": [
    "### 2. Approach\n",
    "\n",
    "- Transform raw text data into input vectors\n",
    "\n",
    "\n",
    "- Implement Feedforward network with\n",
    "    - Input: Embedding weight matrix\n",
    "    - Interim: Hidden layer(S)\n",
    "    - Output: softmax\n",
    "    \n",
    "    \n",
    "- Implement Stochastic Gradient Descent (SGD) algorithm with\n",
    "    - Forward pass to compute intermediate outputs\n",
    "    - Backward pass (Backpropagation) to compute gradients and update weights of the network\n",
    "    - Dropout for regularisation\n",
    "    - Perform hyperparater tuning\n",
    "        - dropout rate\n",
    "        - embedding size\n",
    "        - Learning rate\n",
    "    - Plot the learning process\n",
    "        - training loss\n",
    "        - validation loss\n",
    "        \n",
    "        \n",
    "- Implement pre-trained word embeddings (GloVe)\n",
    "    - Reinitialise network weights with pre-trained word embeddings from GloVe\n",
    "    - Don't update weight during training (weight freezing, NO backpropagation)\n",
    "    - Perform hyperparameter tuning\n",
    "    - Plot the learning process\n",
    "    \n",
    "    \n",
    "- Implement More hidden layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ade1b4",
   "metadata": {},
   "source": [
    "### 3. Dataset\n",
    "\n",
    "#### AG News Corpus\n",
    "- The data is taken from [AG News Corpus](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html).\n",
    "    - Created folder structure:\n",
    "        - `data/train.csv`: contains 2,400 news articles, 800 for each class to be used for training.\n",
    "        - `data/dev.csv`: contains 150 news articles, 50 for each class to be used for hyperparameter selection and monitoring the training process.\n",
    "        - `data/test.csv`: contains 900 news articles, 300 for each class to be used for testing.\n",
    "\n",
    "#### GloVe Embeddings\n",
    "\n",
    "- The pre-trained GloVe embeddings trained on Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download) is available from [here](http://nlp.stanford.edu/data/glove.840B.300d.zip).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9982015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.3.2 in /opt/anaconda3/lib/python3.8/site-packages (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib==3.3.2) (1.19.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib==3.3.2) (8.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib==3.3.2) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib==3.3.2) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib==3.3.2) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib==3.3.2) (2021.10.8)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib==3.3.2) (0.10.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib==3.3.2) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib==3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32f378f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important python libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tnrange\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680754e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "def set_seed():\n",
    "    random.seed(123)\n",
    "    np.random.seed(123)\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b776e3",
   "metadata": {},
   "source": [
    "### Transform raw text data into input vectors\n",
    "\n",
    "- Transform the raw text into train, dev, and test sets\n",
    "- Since the dataset done't have headers, we will first create custome headers and then load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2867f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a customer Header\n",
    "news_header = [\"label\", \"text\"]\n",
    "\n",
    "# Load news data\n",
    "news_train = pd.read_csv('./data/train.csv', names=news_header)\n",
    "news_dev = pd.read_csv('./data/dev.csv', names=news_header)\n",
    "news_test = pd.read_csv('./data/test.csv', names=news_header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c155f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Venezuelans turned out early\\and in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - South Korean police used water canno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Reuters - Thousands of Palestinian\\prisoners i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Sporadic gunfire and shelling took place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>AP - Dozens of Rwandan soldiers flew into Suda...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Reuters - Venezuelans turned out early\\and in ...\n",
       "1      1  Reuters - South Korean police used water canno...\n",
       "2      1  Reuters - Thousands of Palestinian\\prisoners i...\n",
       "3      1  AFP - Sporadic gunfire and shelling took place...\n",
       "4      1  AP - Dozens of Rwandan soldiers flew into Suda..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data\n",
    "news_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e0af3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>BAGHDAD, Iraq - An Islamic militant group that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Parts of Los Angeles international airport are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>AFP - Facing a issue that once tripped up his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The leader of militant Lebanese group Hezbolla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>JAKARTA : ASEAN finance ministers ended a meet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  BAGHDAD, Iraq - An Islamic militant group that...\n",
       "1      1  Parts of Los Angeles international airport are...\n",
       "2      1  AFP - Facing a issue that once tripped up his ...\n",
       "3      1  The leader of militant Lebanese group Hezbolla...\n",
       "4      1  JAKARTA : ASEAN finance ministers ended a meet..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dev.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c9b755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Canadian Press - VANCOUVER (CP) - The sister o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AP - The man who claims Gov. James E. McGreeve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>NAJAF, Iraq - Explosions and gunfire rattled t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>LOURDES, France - A frail Pope John Paul II, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Supporters and rivals warn of possible fraud; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      1  Canadian Press - VANCOUVER (CP) - The sister o...\n",
       "1      1  AP - The man who claims Gov. James E. McGreeve...\n",
       "2      1  NAJAF, Iraq - Explosions and gunfire rattled t...\n",
       "3      1  LOURDES, France - A frail Pope John Paul II, b...\n",
       "4      1  Supporters and rivals warn of possible fraud; ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630aabb0",
   "metadata": {},
   "source": [
    "Separate the features and lables lists for train, dev, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e35c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_Xtrain = news_train['text'].tolist()\n",
    "news_Xdev = news_dev['text'].tolist()\n",
    "news_Xtest = news_test['text'].tolist()\n",
    "\n",
    "news_ytrain = news_train['label'].to_numpy()\n",
    "news_ydev = news_dev['label'].to_numpy()\n",
    "news_ytest = news_test['label'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "783fb0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "2400\n",
      "150\n",
      "150\n",
      "900\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the new datasets\n",
    "print(len(news_Xtrain))\n",
    "print(len(news_ytrain))\n",
    "print(len(news_Xdev))\n",
    "print(len(news_ydev))\n",
    "print(len(news_Xtest))\n",
    "print(len(news_ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535fdb8",
   "metadata": {},
   "source": [
    "### Implement Feedforward network\n",
    "\n",
    "**Create input representations**\n",
    "- To build a Feedforward neural network, the input data representation needs to be obtained in two ways; from vocabulary or from one-hot encoding. Since, one-hot encoding reqires large memory capacity, we ill repreent the input documents as a list of vocabulary indices where each wrod corresponds to a vocabulary index.\n",
    "\n",
    "**Text pre-processing pipeline**\n",
    "- To obtrain the vocabulary of words, we will create text pre-processing pipeline as below:\n",
    "    - Tokenise all texts into a list of unigrams\n",
    "    - Remove stop words\n",
    "    - Remove unigrams appearing in less than K documents\n",
    "    - Use the remaining unigrams to create the vocabulary of top-N most frequent unigrams in the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0378a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the stop words list\n",
    "stop_words = ['a', 'i', \"i'm\", 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', \n",
    "              'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', \n",
    "              'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", \n",
    "              'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "              'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', \n",
    "              'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', \n",
    "              'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', \n",
    "              'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', \n",
    "              'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', \n",
    "              'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n",
    "              'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', \n",
    "              'each', 'few', 'more', 'most', 'other', 'some', 'such', \n",
    "              'only', 'own', 'same', 'so', 'than', 'too', 'very', 'can', 'will', \n",
    "              'just', 'should', \"should've\", 'now', 'll', 're', \n",
    "              've', 'ma', \".\", \",\", \"would\", \"could\", \"must\", \"shall\", \"may\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75790a2c",
   "metadata": {},
   "source": [
    "### Unigram extraction from a document\n",
    "\n",
    "\n",
    "Implement ngram extraction function `extract_ngrams` where \n",
    "- `x_raw`: a string corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams being extracted (e.g. (1,2) denotes extracting unigrams and bigrams)\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that the data is already tokenised so we will use white space tokenisation\n",
    "- `stop_words`: a list of stop words\n",
    "- `vocab`: a given vocabulary. It should be used to extract specific features\n",
    "    \n",
    "and return \n",
    "- list of all extracted features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad2eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement ngram extraction\n",
    "def extract_ngrams(x_raw, ngram_range=(1,3), \n",
    "                  token_pattern=f'\\b[A-Za-z][A-Za-z]+\\b',\n",
    "                 stop_words=[], vocab=set()):\n",
    "    token_regex = re.compile(token_pattern)\n",
    "    \n",
    "    # Extract all unigrams by tokenising\n",
    "    x_unigram = [w for w in token_regex.findall(str(x_raw).lower(),) if w not in stop_words]\n",
    "    \n",
    "    # Store the unigrams to be returned\n",
    "    x = []\n",
    "    if ngram_range[0] == 1:\n",
    "        x = x_unigram\n",
    "    \n",
    "    # Generate n-grams from avaialbe unigrams\n",
    "    ngrams = []\n",
    "    for n in range(ngram_range[0], ngram_range[1]+1):\n",
    "        if n==1: \n",
    "            continue\n",
    "        \n",
    "        # Pass a list of lists as an argument for zip\n",
    "        arg_list = [x_unigram]+[x_unigram[i:] for i in range(1, n)]\n",
    "        \n",
    "        # extract tuples of n-grams using zip\n",
    "        # for bigram this should look: list(zip(x_uni, x_uni[1:]))\n",
    "        # align each item x[i] in x_uni with the next one x[i+1]. \n",
    "        # Note that x_uni and x_uni[1:] have different lenghts\n",
    "        # but zip ignores redundant elements at the end of the second list\n",
    "        # Alternatively, this could be done with for loops\n",
    "        x_ngram = list(zip(*arg_list))\n",
    "        ngrams.append(x_ngram)\n",
    "    \n",
    "    for n in ngrams:\n",
    "        for t in n:\n",
    "            x.append(t)\n",
    "            \n",
    "    if len(vocab)>0:\n",
    "        x=[w for w in x if w in vocab]\n",
    "    \n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a65971",
   "metadata": {},
   "source": [
    "### Create a vocabular of n-grams\n",
    "\n",
    "Implement `get_vocab` function, where\n",
    "- `X_raw`: a list of strings each corresponding to the raw text of a document\n",
    "- `ngram_range`: a tuple of two integers denoting the type of ngrams being extracted (e.g. (1,2) denotes extracting unigrams and bigrams)\n",
    "- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised\n",
    "- `stop_words`: a list of stop words\n",
    "- `min_df`: keep ngrams with a minimum document frequency\n",
    "- `keep_topN`: keep top-N more frequent ngrams\n",
    "\n",
    "and return\n",
    "- `vocab`: a set of n-grams that will be used as features\n",
    "- `df`: a document frequency dictionay that contains ngrams as key and their corresponding document frequenciy as values\n",
    "- `ngram_counts`: counts of each ngram in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a543352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(X_raw, ngram_range=(1,3),\n",
    "             token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',\n",
    "             min_df=0, keep_topN=0, stop_words=[]):\n",
    "\n",
    "    token_regex = re.compile(token_pattern)\n",
    "    df = Counter()\n",
    "    ngram_counts = Counter()\n",
    "    vocab = set()\n",
    "    \n",
    "    # Iterate through each raw text\n",
    "    for x in X_raw:\n",
    "        x_ngram = extract_ngrams(x, ngram_range=ngram_range, \n",
    "                                 token_pattern=token_pattern, \n",
    "                                 stop_words=stop_words)\n",
    "        # Update document frequency and ngram count\n",
    "        df.update(list(set(x_ngram)))\n",
    "        ngram_counts.update(x_ngram)\n",
    "        \n",
    "    # Obtain vocabulary as a set with document frequency > min document frequency\n",
    "    vocab = set([w for w in df if df[w]>=min_df])\n",
    "    \n",
    "    # Keep the top N most frequent vocab\n",
    "    if keep_topN:\n",
    "        vocab =set([w[0] for w in ngram_counts.most_common(keep_topN)\n",
    "                   if w[0] in vocab])\n",
    "        \n",
    "    return vocab, df, ngram_counts\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e55ba4",
   "metadata": {},
   "source": [
    "### Get the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4ad02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news vocab size: 2000\n",
      "['nestle', 'take', 'walked', 'gone', 'end', 'tickets', 'wall', 'hand', 'billion', 'selection', 'snow', 'mariel', 'proposal', 'figures', 'sources', 'expected', 'day', 'delay', 'center', 'better', 'region', 'captain', 'jersey', 'cost', 'hamid', 'july', 'heard', 'get', 'defeat', 'india', 'tomas', 'president', 'thanou', 'microsoft', 'tenders', 'opponent', 'residents', 'tony', 'cross', 'auction', 'pieter', 'justin', 'job', 'costs', 'rival', 'czech', 'moderate', 'soldier', 'halliburton', 'main', 'decade', 'programs', 'struggled', 'states', 'boxing', 'survived', 'amp', 'part', 'silver', 'philippines', 'earnings', 'kuala', 'coming', 'battled', 'test', 'costco', 'australia', 'following', 'canadian', 'florida', 'phelps', 'ways', 'super', 'affair', 'kept', 'sixth', 'israeli', 'quarter', 'friends', 'hong', 'andy', 'improved', 'large', 'linked', 'advanced', 'market', 'rest', 'pass', 'criticism', 'died', 'minister', 'punta', 'response', 'francisco', 'kobe', 'defense', 'east', 'bloomberg', 'eased', 'image', 'press', 'millions', 'appeared', 'federal', 'denied', 'shares', 'beat', 'assault', 'lead', 'help', 'throw', 'dispute', 'strained', 'lows', 'miles', 'prepared', 'darfur', 'red', 'interim', 'kid', 'backed', 'insurer', 'capacity', 'violation', 'eagerly', 'beating', 'olympic', 'time', 'positions', 'agent', 'cincinnati', 'got', 'table', 'within', 'prices', 'years', 'front', 'required', 'slower', 'lines', 'citizens', 'big', 'fractured', 'administration', 'consider', 'pitch', 'surgery', 'easy', 'battle', 'taking', 'athletics', 'barclays', 'revolution', 'southern', 'branch', 'operative', 'streak', 'legal', 'awaited', 'probably', 'cp', 'yasser', 'manage', 'story', 'fueled', 'care', 'important', 'planned', 'united', 'jason', 'sec', 'declined', 'running', 'edwards', 'conference', 'protests', 'round', 'surge', 'inevitable', 'ticker', 'ite', 'militiamen', 'trap', 'unit', 'wal', 'improve', 'disappeared', 'tax', 'rejected', 'flew', 'money', 'seen', 'showing', 'innings', 'back', 'howard', 'wide', 'potential', 'comes', 'fight', 'public', 'materials', 'accept', 'attack', 'starts', 'want', 'jump', 'police', 'quot', 'served', 'sweep', 'congolese', 'release', 'killed', 'voted', 'giants', 'coast', 'member', 'faster', 'militant', 'appeal', 'airways', 'hopes', 'terrorists', 'problems', 'negotiate', 'injured', 'facilities', 'cbs', 'showed', 'key', 'disaster', 'bell', 'afghanistan', 'fund', 'wing', 'greeks', 'stewart', 'reversed', 'field', 'mason', 'world', 'lawsuit', 'reserve', 'campaign', 'toronto', 'extra', 'military', 'moved', 'dismissed', 'personal', 'four', 'kitajima', 'summer', 'bid', 'still', 'growth', 'championship', 'chairman', 'funds', 'airline', 'tumbled', 'small', 'sprinter', 'either', 'athletes', 'refused', 'england', 'injuries', 'went', 'data', 'promised', 'sweeping', 'australian', 'prison', 'june', 'wake', 'general', 'usa', 'outside', 'forces', 'children', 'watch', 'claims', 'satellite', 'dow', 'defensive', 'northeast', 'kostas', 'fischer', 'results', 'performance', 'meter', 'typhoon', 'captured', 'sentiment', 'found', 'afp', 'improvement', 'wis', 'quit', 'fired', 'opener', 'complete', 'jackson', 'michael', 'settler', 'breakaway', 'holiday', 'witnesses', 'offering', 'wednesday', 'williams', 'eight', 'japan', 'pakistan', 'led', 'serving', 'russia', 'knee', 'swimmer', 'credit', 'helicopter', 'available', 'situation', 'per', 'sense', 'decline', 'computer', 'prosecutor', 'overnight', 'uae', 'burundi', 'watson', 'latham', 'series', 'berlin', 'expansion', 'oakland', 'weapons', 'finished', 'rico', 'thousands', 'party', 'period', 'ryder', 'isn', 'farm', 'swimming', 'operating', 'href', 'moqtada', 'silvio', 'charges', 'survey', 'conspiring', 'ny', 'games', 'delayed', 'roddick', 'violations', 'suffered', 'minute', 'explosion', 'cars', 'system', 'arafat', 'aimed', 'cut', 'robert', 'outlooks', 'enter', 'sanctions', 'revenue', 'fre', 'summit', 'southwest', 'teenager', 'case', 'hungarian', 'believes', 'toward', 'foot', 'independence', 'shrine', 'enough', 'proved', 'selling', 'falconio', 'support', 'hitting', 'draft', 'free', 'development', 'operation', 'detroit', 'english', 'charley', 'corporate', 'three', 'rookie', 'like', 'floods', 'see', 'posted', 'surging', 'airport', 'showdown', 'young', 'try', 'accounting', 'heavy', 'stop', 'oust', 'coach', 'europe', 'changed', 'upset', 'afghan', 'german', 'patriots', 'scheduled', 'champion', 'quarterback', 'overboard', 'losing', 'outfielder', 'practice', 'initial', 'place', 'commission', 'prime', 'recall', 'activity', 'hold', 'islamic', 'group', 'berlusconi', 'earlier', 'protect', 'sign', 'next', 'prisoners', 'efforts', 'cause', 'sending', 'half', 'portugal', 'positive', 'nations', 'woods', 'straight', 'maker', 'mike', 'olympia', 'ohio', 'registration', 'returning', 'growing', 'vegas', 'evening', 'suspended', 'ravaged', 'reuters', 'xinhuanet', 'nikkei', 'white', 'asia', 'arlington', 'rates', 'euro', 'grant', 'giving', 'governor', 'ranked', 'regular', 'cleric', 'investors', 'crowded', 'reds', 'gain', 'aspx', 'ninth', 'holiest', 'bank', 'stand', 'terrorist', 'little', 'full', 'people', 'lawyers', 'companies', 'join', 'says', 'occupied', 'netherlands', 'sports', 'congress', 'detained', 'won', 'expects', 'catch', 'doping', 'trees', 'greek', 'us', 'concern', 'given', 'pre', 'late', 'payment', 'yet', 'minnesota', 'hitter', 'moves', 'build', 'fourth', 'product', 'executive', 'iverson', 'trading', 'torture', 'though', 'jumped', 'puerto', 'ukraine', 'bought', 'motorola', 'grew', 'cold', 'largest', 'pitcher', 'preliminary', 'might', 'boost', 'medallist', 'rangers', 'show', 'andrew', 'leave', 'supplies', 'path', 'billing', 'ticket', 'internet', 'wreckage', 'early', 'reach', 'retailer', 'countries', 'scotland', 'self', 'counting', 'pm', 'troops', 'attacks', 'wind', 'venezuelans', 'movement', 'kong', 'goal', 'carrier', 'slow', 'meet', 'truce', 'string', 'find', 'idea', 'dream', 'national', 'known', 'bids', 'light', 'short', 'high', 'registered', 'oil', 'check', 'left', 'return', 'negotiations', 'needs', 'fighters', 'night', 'threat', 'borders', 'well', 'didn', 'assistant', 'course', 'finals', 'reform', 'cents', 'official', 'qaida', 'toyota', 'regional', 'workers', 'rebels', 'defused', 'foreign', 'office', 'even', 'thanks', 'freestyle', 'talks', 'peace', 'alexander', 'oldest', 'bj', 'dawn', 'strip', 'war', 'commerce', 'rolled', 'tearing', 'promotion', 'indian', 'markets', 'numbers', 'area', 'suspected', 'retail', 'tourist', 'giant', 'least', 'clemens', 'raids', 'conflict', 'katerina', 'tied', 'hits', 'earned', 'tens', 'soccer', 'goog', 'israel', 'laws', 'nigeria', 'post', 'right', 'separatist', 'economic', 'struggling', 'rules', 'battles', 'dozens', 'practices', 'miracle', 'prominent', 'pick', 'highs', 'dozen', 'park', 'keller', 'others', 'labor', 'great', 'refinancings', 'presence', 'although', 'climbed', 'intelligence', 'online', 'seed', 'chief', 'preliminaries', 'jerusalem', 'mdt', 'suspicious', 'taken', 'amateur', 'nelson', 'topping', 'mark', 'john', 'luxury', 'drive', 'winds', 'improvements', 'judo', 'americans', 'stunning', 'recovering', 'schroeder', 'reached', 'ever', 'embassy', 'chavez', 'confidence', 'worst', 'urban', 'carter', 'global', 'ditch', 'bill', 'arbitration', 'whether', 'settlement', 'quote', 'slashed', 'site', 'radioactive', 'singles', 'past', 'every', 'across', 'created', 'dollar', 'pulled', 'tech', 'team', 'landmark', 'boston', 'upbeat', 'black', 'vijay', 'finish', 'league', 'opened', 'recovery', 'lumpur', 'moscow', 'georgian', 'profile', 'chance', 'kevin', 'slam', 'las', 'mortgage', 'finally', 'joining', 'exports', 'rather', 'delivered', 'wisconsin', 'greece', 'industry', 'used', 'major', 'game', 'tie', 'around', 'huge', 'tanks', 'points', 'profit', 'air', 'saying', 'average', 'mcdonald', 'diego', 'block', 'increasing', 'decided', 'rose', 'mandate', 'including', 'officials', 'saw', 'collect', 'houston', 'challenges', 'kuwait', 'germany', 'low', 'chip', 'not', 'edge', 'caused', 'lives', 'spending', 'successful', 'matter', 'size', 'killing', 'scott', 'paperwork', 'ahead', 'third', 'sent', 'offers', 'girl', 'shoulder', 'ten', 'madrid', 'bills', 'opposition', 'keep', 'output', 'finance', 'took', 'faces', 'reduction', 'reforms', 'pull', 'judges', 'net', 'senior', 'ministry', 'claiming', 'wild', 'quincy', 'failed', 'bomb', 'pinch', 'knows', 'hellip', 'class', 'renewed', 'question', 'semifinals', 'private', 'cease', 'italy', 'men', 'association', 'convicted', 'qualifying', 'call', 'auditors', 'un', 'chess', 'parmalat', 'increased', 'gas', 'malaysia', 'equity', 'lay', 'sex', 'heart', 'www', 'bringing', 'street', 'whose', 'effective', 'medal', 'spitz', 'relating', 'history', 'loans', 'income', 'leftist', 'forecast', 'sun', 'quest', 'african', 'college', 'financial', 'guillen', 'violent', 'janus', 'best', 'revealed', 'mutual', 'programme', 'atlanta', 'cos', 'season', 'disruptions', 'bronze', 'union', 'caracas', 'came', 'along', 'putter', 'championships', 'commit', 'ratings', 'jury', 'homes', 'ap', 'hurt', 'teammate', 'champions', 'teixeira', 'plc', 'looking', 'venezuelan', 'suspects', 'among', 'taxes', 'cycling', 'illinois', 'alleged', 'plans', 'move', 'ipo', 'country', 'mass', 'georgia', 'yankees', 'claim', 'ancient', 'level', 'british', 'opening', 'hal', 'stores', 'buy', 'familiar', 'replaced', 'sri', 'use', 'hope', 'custody', 'powerful', 'david', 'closely', 'wave', 'hamilton', 'deficit', 'things', 'bowl', 'another', 'minutes', 'exchange', 'turkish', 'massacre', 'poor', 'butterfly', 'democracy', 'middle', 'stay', 'fall', 'consumer', 'fighting', 'last', 'sudan', 'capital', 'web', 'cowboys', 'week', 'hotel', 'powder', 'start', 'jay', 'starting', 'kansas', 'view', 'jcp', 'korea', 'easing', 'thing', 'stake', 'term', 'futures', 'ibm', 'strong', 'needed', 'bujumbura', 'tutsi', 'days', 'expectations', 'style', 'delivery', 'tournament', 'agency', 'let', 'debt', 'sardinia', 'seattle', 'produced', 'good', 'torn', 'becoming', 'future', 'name', 'opec', 'fla', 'afternoon', 'maktoum', 'estimated', 'cash', 'leonard', 'hot', 'helped', 'bryant', 'rally', 'sad', 'yard', 'protest', 'tests', 'reportedly', 'gains', 'medical', 'extending', 'later', 'said', 'decision', 'convention', 'twins', 'breaking', 'trade', 'crown', 'law', 'brought', 'straits', 'ian', 'basketball', 'interview', 'massachusetts', 'hundreds', 'missed', 'muqtada', 'grim', 'google', 'concerns', 'iraq', 'knew', 'surged', 'flood', 'broke', 'chinese', 'child', 'tim', 'maria', 'power', 'tour', 'rwandan', 'row', 'continue', 'ex', 'order', 'device', 'trying', 'lourdes', 'dug', 'almost', 'morning', 'become', 'conspiracy', 'insurance', 'kosuke', 'weaker', 'play', 'texas', 'ossetia', 'football', 'force', 'version', 'go', 'already', 'tools', 'iraqi', 'offset', 'record', 'co', 'ariel', 'crash', 'tore', 'seeking', 'pushed', 'buildings', 'outlook', 'rights', 'bring', 'st', 'look', 'resolve', 'testing', 'slumping', 'regulatory', 'island', 'twenty', 'seven', 'individual', 'suspend', 'families', 'lt', 'changes', 'los', 'visit', 'food', 'beaten', 'ease', 'criminal', 'fellow', 'rushed', 'yesterday', 'civil', 'instead', 'envoy', 'remain', 'nine', 'johnson', 'highly', 'korean', 'quarterly', 'managers', 'vault', 'step', 'arch', 'touted', 'hockey', 'soaring', 'rise', 'verge', 'accused', 'officer', 'ramallah', 'haiti', 'school', 'treasury', 'mat', 'university', 'securities', 'focus', 'sudanese', 'electoral', 'monthly', 'ending', 'mobile', 'two', 'guard', 'indians', 'vice', 'safety', 'injury', 'nation', 'africa', 'fear', 'china', 'fleet', 'road', 'bay', 'bonds', 'agreement', 'tomorrow', 'organization', 'approval', 'hostile', 'cuts', 'effort', 'set', 'kmrt', 'premier', 'calm', 'becomes', 'committee', 'ireland', 'fifth', 'blair', 'march', 'hall', 'search', 'hilton', 'months', 'gaza', 'final', 'hours', 'boys', 'health', 'pontiff', 'raise', 'ended', 'housing', 'closing', 'single', 'pentagon', 'family', 'withdrew', 'demand', 'congestion', 'devices', 'missing', 'tuesday', 'hour', 'embarrassing', 'leaders', 'facing', 'say', 'makers', 'collective', 'aside', 'ii', 'paid', 'grand', 'factories', 'scandal', 'gap', 'received', 'playing', 'quickinfo', 'energy', 'winged', 'antonio', 'point', 'judge', 'plus', 'liverpool', 'arizona', 'called', 'without', 'teammates', 'nearly', 'dimarco', 'communist', 'long', 'much', 'security', 'hugo', 'behind', 'tom', 'department', 'deal', 'investigation', 'leg', 'baseman', 'stadium', 'equipment', 'supply', 'total', 'jose', 'released', 'par', 'victims', 'watched', 'japanese', 'vs', 'former', 'tested', 'launched', 'london', 'homer', 'pittsburgh', 'venezuela', 'became', 'bhp', 'six', 'gymnasts', 'joined', 'increase', 'thursday', 'started', 'payments', 'america', 'stopped', 'blue', 'appealed', 'industrial', 'dell', 'creating', 'sell', 'signing', 'groups', 'accepted', 'losses', 'washington', 'stayed', 'serious', 'dead', 'threatened', 'annual', 'prove', 'louis', 'nuclear', 'near', 'paul', 'alert', 'com', 'mining', 'insurers', 'thumb', 'mac', 'crowd', 'way', 'villa', 'talk', 'romania', 'track', 'tennis', 'breaststroke', 'electricity', 'switzerland', 'town', 'controversial', 'largely', 'jean', 'inc', 'shot', 'militia', 'elections', 'clashes', 'amat', 'determine', 'boosted', 'brian', 'authority', 'professional', 'rebel', 'philadelphia', 'bankruptcy', 'flu', 'oq', 'corruption', 'russian', 'shi', 'result', 'media', 'midday', 'american', 'also', 'board', 'expos', 'published', 'beginning', 'opponents', 'brokerage', 'aware', 'due', 'gold', 'toll', 'women', 'rebounded', 'hard', 'arrived', 'airlines', 'historic', 'added', 'ltd', 'fire', 'named', 'meetings', 'employees', 'fresh', 'kerry', 'playoff', 'activists', 'presidential', 'executives', 'session', 'range', 'aides', 'match', 'business', 'thought', 'inflation', 'singh', 'home', 'several', 'members', 'green', 'motors', 'index', 'waiting', 'invoices', 'exporters', 'statement', 'terrorism', 'loyal', 'kmart', 'bases', 'multi', 'raised', 'many', 'montreal', 'separate', 'storm', 'crude', 'claimed', 'city', 'international', 'falling', 'sea', 'daily', 'leading', 'areas', 'relay', 'made', 'blow', 'preseason', 'van', 'water', 'anti', 'ago', 'secretary', 'shooting', 'chain', 'commodities', 'winning', 'dollars', 'presidency', 'ambassador', 'paris', 'mistakes', 'elbow', 'news', 'shiite', 'year', 'son', 'singapore', 'citing', 'nasdaq', 'getting', 'sport', 'advertising', 'trial', 'withdraw', 'share', 'highest', 'gathering', 'hottest', 'gave', 'county', 'jays', 'likely', 'thorpe', 'drug', 'depot', 'times', 'policy', 'benefits', 'el', 'baghdad', 'torri', 'signs', 'star', 'bit', 'canada', 'higher', 'hands', 'struck', 'store', 'athens', 'brazil', 'believe', 'auto', 'winner', 'rain', 'always', 'kenteris', 'freddie', 'inflationary', 'assembly', 'regulators', 'involving', 'make', 'need', 'based', 'bad', 'doubts', 'emergency', 'worries', 'western', 'expressed', 'phone', 'steve', 'close', 'britain', 'withhold', 'picked', 'event', 'panel', 'backpacker', 'golf', 'biggest', 'come', 'prompted', 'jewish', 'heat', 'fell', 'palestinian', 'tokyo', 'trouble', 'rod', 'defence', 'northern', 'newly', 'announcement', 'victory', 'slump', 'hurricane', 'landing', 'mariners', 'fraud', 'wants', 'concerned', 'bargain', 'sprinters', 'give', 'doubled', 'happen', 'begins', 'lost', 'sold', 'urged', 'advance', 'month', 'baseball', 'sunday', 'jr', 'zealand', 'congo', 'larry', 'cent', 'investor', 'living', 'analysts', 'challenge', 'gerhard', 'latest', 'saudi', 'businesses', 'head', 'information', 'man', 'control', 'source', 'building', 'cleveland', 'allowed', 'north', 'classic', 'uk', 'signed', 'austria', 'real', 'barrel', 'stage', 'billions', 'send', 'prosecutors', 'apparently', 'avoid', 'suspension', 'prescription', 'chris', 'reports', 'local', 'fought', 'gasoline', 'ground', 'billiton', 'teams', 'peter', 'charged', 'november', 'hd', 'wife', 'iran', 'authorities', 'political', 'cities', 'continued', 'woman', 'ask', 'disciplinary', 'ing', 'kingdom', 'met', 'hoping', 'recent', 'ethnic', 'meeting', 'refugee', 'company', 'eastern', 'title', 'calls', 'nepal', 'million', 'fears', 'face', 'software', 'estimates', 'stocks', 'turned', 'province', 'sydney', 'quickly', 'special', 'hoogenband', 'mart', 'ordered', 'democratic', 'pool', 'perhaps', 'prince', 'done', 'sprint', 'october', 'holy', 'defending', 'invest', 'asian', 'drop', 'uprising', 'venus', 'arms', 'lowest', 'metres', 'parade', 'clear', 'gymnastics', 'second', 'banned', 'friendly', 'den', 'inning', 'tight', 'unrest', 'rule', 'al', 'host', 'human', 'ag', 'senate', 'win', 'cardinals', 'attempts', 'maoist', 'weekly', 'announced', 'issued', 'sharon', 'leader', 'closer', 'banks', 'bush', 'border', 'ottawa', 'details', 'hamas', 'hearing', 'crashed', 'stock', 'chicago', 'television', 'told', 'sale', 'costas', 'jail', 'draw', 'immediately', 'filed', 'heading', 'meters', 'saturday', 'hansen', 'threatening', 'top', 'candidate', 'firm', 'jamaica', 'angeles', 'investment', 'flash', 'south', 'strike', 'divisions', 'twice', 'away', 'refugees', 'impact', 'crisis', 'driven', 'telephone', 'manager', 'begin', 'debut', 'cautious', 'popular', 'george', 'fuel', 'profits', 'attempt', 'gary', 'zagunis', 'voters', 'spot', 'roger', 'brown', 'working', 'corp', 'berdych', 'chancellor', 'tutsis', 'herat', 'jobs', 'supporters', 'price', 'terror', 'drove', 'forward', 'dallas', 'sharply', 'militants', 'armed', 'sharp', 'nfl', 'offered', 'religious', 'rate', 'consecutive', 'miss', 'eliminated', 'restaurants', 'setting', 'phillies', 'open', 'faced', 'rathore', 'adopted', 'globe', 'far', 'republican', 'buried', 'rivals', 'disarm', 'abroad', 'wounding', 'conditions', 'held', 'european', 'players', 'york', 'something', 'weekend', 'royal', 'yukos', 'bgp', 'beijing', 'runs', 'motor', 'rising', 'spokesman', 'khartoum', 'closed', 'events', 'euros', 'haas', 'plan', 'knocked', 'colorado', 'august', 'legg', 'soon', 'interest', 'person', 'sadr', 'strength', 'raising', 'french', 'break', 'islamabad', 'applied', 'sales', 'http', 'homered', 'amid', 'roman', 'kathmandu', 'california', 'warned', 'lower', 'diplomatic', 'division', 'calif', 'hare', 'competition', 'brendan', 'production', 'today', 'weeks', 'first', 'network', 'bargaining', 'calling', 'earn', 'votes', 'government', 'hospitals', 'population', 'loss', 'blockade', 'no', 'since', 'ailing', 'contract', 'haven', 'began', 'products', 'dodgers', 'arab', 'approve', 'hospital', 'fed', 'nortel', 'player', 'qaeda', 'continues', 'adam', 'monday', 'together', 'training', 'career', 'italian', 'firms', 'damage', 'report', 'rare', 'caught', 'target', 'according', 'newspaper', 'gov', 'pay', 'gorda', 'settlements', 'referendum', 'lack', 'double', 'issues', 'radical', 'bobby', 'acquisition', 'appears', 'looks', 'house', 'anticipated', 'current', 'orioles', 'card', 'james', 'gained', 'associated', 'walk', 'race', 'violence', 'olympics', 'death', 'fullquote', 'questions', 'friday', 'program', 'vote', 'drew', 'kabul', 'election', 'fans', 'helping', 'reported', 'palestinians', 'ban', 'applications', 'hit', 'construction', 'networks', 'previous', 'lift', 'possible', 'massive', 'offer', 'percent', 'fast', 'put', 'declare', 'de', 'radio', 'eighth', 'customers', 'west', 'less', 'pro', 'club', 'delegates', 'arrested', 'merger', 'pitched', 'upi', 'rescue', 'camp', 'remote', 'action', 'operator', 'making', 'work', 'inquiry', 'september', 'dropped', 'troubled', 'evidence', 'central', 'really', 'allen', 'magazine', 'wholesale', 'declared', 'run', 'swept', 'boy', 'change', 'akron', 'technology', 'old', 'sox', 'father', 'soldiers', 'hunger', 'village', 'warning', 'whistling', 'owners', 'launch', 'pressure', 'despite', 'process', 'slip', 'new', 'played', 'management', 'service', 'najaf', 'approved', 'asked', 'side', 'tiger', 'fencing', 'research', 'picture', 'army', 'operations', 'lowe', 'gt', 'compete', 'buyers', 'republic', 'crowds', 'holding', 'charge', 'nyse', 'going', 'relief', 'pile', 'staff', 'klete', 'muslim', 'life', 'headed', 'sept', 'psychological', 'pope', 'agreed', 'shia', 'services', 'pga', 'lose', 'san', 'arena', 'exhibition', 'aug', 'one', 'hamm', 'desperate', 'wanted', 'protesters', 'mary', 'sick', 'number', 'levels', 'rallied', 'targets', 'reduced', 'port', 'engine', 'state', 'jones', 'diving', 'federer', 'plotting', 'france', 'base', 'warnings', 'volleyball', 'five', 'scored', 'recently', 'medals', 'wounded', 'band', 'line', 'leaving', 'court', 'rebound', 'economy', 'cup', 'observers', 'tell', 'karzai', 'inventory', 'body', 'baltimore', 'shrugged', 'car', 'drugs', 'speed', 'murder', 'delegation', 'sites']\n"
     ]
    }
   ],
   "source": [
    "keep_topN = 2000\n",
    "ngram_range = (1,1)\n",
    "min_df = 0\n",
    "(vocab_set,\n",
    "df,\n",
    "ngram_counts) = get_vocab(news_Xtrain, ngram_range=ngram_range,\n",
    "                          min_df=min_df, keep_topN=keep_topN,\n",
    "                          stop_words=stop_words)\n",
    "news_vocab = list(vocab_set)\n",
    "print(f'news vocab size: {len(news_vocab)}')\n",
    "print(news_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e62669",
   "metadata": {},
   "source": [
    "As far as vocabulary size is concerned, the larger vocabulary gives larger breadth of words to be picked up during training. However, for this experiment, I have restricted to size 2000. In the upcoming section, we will do hyperparameter tuning during training to get the reasonable accuracy while using this vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4a6f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word to id dictionary for future use\n",
    "word2id = dict(zip(news_vocab, range(len(news_vocab))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db1980",
   "metadata": {},
   "source": [
    "### Convert the list of unigrams  into a list of vocabulary indices\n",
    "- Storing actual one-hot vectors into memory for all words in the entire data set is prohibitive. Instead, we will store word indices in the vocabulary and look-up the weight matrix. This is equivalent of doing a dot product between an one-hot vector and the weight matrix. \n",
    "\n",
    "- Implement a function `extract_ngrams_for_train_dev_test_sets` to represent documents in train, dev and test sets as lists of words in the vocabulary, where\n",
    "    - `ngram_range`: a tuple of two integers denoting the type of ngrams being extracted (e.g. (1,2) denotes extracting unigrams and bigrams)\n",
    "    - `vocab`: a vocabulary set\n",
    "- and returns\n",
    "    - `Xtrain_ngrams`: ngrams for train set\n",
    "    - `Xdev_ngrams`: ngrams for dev set\n",
    "    - `Xtest_ngrams`: ngrams for test set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "409dca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams_for_train_dev_test_sets(ngram_range, vocab):\n",
    "    Xtrain = news_Xtrain\n",
    "    Xdev   = news_Xdev\n",
    "    Xtest  = news_Xtest\n",
    "    \n",
    "    # Extract n-grams for training set\n",
    "    Xtrain_ngrams = list()\n",
    "    for i in tnrange(len(Xtrain)):\n",
    "        Xtrain_ngrams.append(extract_ngrams(Xtrain[i], \n",
    "                                            ngram_range=ngram_range, token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',\n",
    "                                            stop_words=stop_words, vocab=vocab))\n",
    "    # Extract n-grams for development set\n",
    "    Xdev_ngrams = list()\n",
    "    for i in tnrange(len(Xdev)):\n",
    "        Xdev_ngrams.append(extract_ngrams(Xdev[i], \n",
    "                                            ngram_range=ngram_range, token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',\n",
    "                                            stop_words=stop_words, vocab=vocab))\n",
    "    # Extract n-grams for test set\n",
    "    Xtest_ngrams = list()\n",
    "    for i in tnrange(len(Xtest)):\n",
    "        Xtest_ngrams.append(extract_ngrams(Xtest[i], \n",
    "                                            ngram_range=ngram_range, token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',\n",
    "                                            stop_words=stop_words, vocab=vocab))\n",
    "        \n",
    "    return Xtrain_ngrams, Xdev_ngrams, Xtest_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de58aa4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cf0f96797c432f832a8c1c2c1ebc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1955be37de1349b4a0dbf47dab83463f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaf84277dea4eefb773f41c4063ac44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(news_Xtrain_ngrams, \n",
    " news_Xdev_ngrams, \n",
    " news_Xtest_ngrams) = extract_ngrams_for_train_dev_test_sets(ngram_range, news_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b037567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into list of indices\n",
    "news_Xtrain_ngrams_indices = []\n",
    "for ngrams in news_Xtrain_ngrams:\n",
    "    news_Xtrain_ngrams_indices.append([word2id.get(ngram, -1) for ngram in ngrams])\n",
    "    \n",
    "news_Xdev_ngrams_indices = []\n",
    "for ngrams in news_Xdev_ngrams:\n",
    "    news_Xdev_ngrams_indices.append([word2id.get(ngram, -1) for ngram in ngrams])\n",
    "\n",
    "news_Xtest_ngrams_indices = []\n",
    "for ngrams in news_Xtest_ngrams:\n",
    "    news_Xtest_ngrams_indices.append([word2id.get(ngram, -1) for ngram in ngrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "859cd03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reuters', 'venezuelans', 'turned', 'early', 'large', 'numbers', 'sunday', 'vote', 'historic', 'referendum', 'either', 'left', 'wing', 'president', 'hugo', 'chavez', 'office', 'give', 'new', 'mandate', 'next', 'two', 'years']\n",
      "[463, 557, 1567, 546, 82, 619, 1490, 1836, 1318, 1810, 264, 578, 235, 31, 1201, 688, 601, 1480, 1909, 756, 441, 1113, 135]\n"
     ]
    }
   ],
   "source": [
    "# check the sample of ngram and corresponging vocabulary index\n",
    "print(news_Xtrain_ngrams[0])\n",
    "print(news_Xtrain_ngrams_indices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61029c05",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network Architecture\n",
    "\n",
    "The steps to create Feedforward neural network as as following:\n",
    "\n",
    "- Pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n",
    "\n",
    "$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n",
    "\n",
    "where $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n",
    "\n",
    "- Then $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n",
    "\n",
    "$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n",
    "\n",
    "- Finally the hidden layer is passed to the output layer:\n",
    "\n",
    "\n",
    "$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W) $$ \n",
    "where $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n",
    "\n",
    "- During training, $\\mathbf{a}_1$ is going to be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n",
    "\n",
    "- The network will also be extended to a deeper architecture by passing a hidden layer to another one:\n",
    "\n",
    "$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i $$\n",
    "\n",
    "$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95050bfb",
   "metadata": {},
   "source": [
    "Create a new function called `get_weight_matrix` which will be used to initialise weight matrix for Feedforward nueral network. The function takes as input:\n",
    "\n",
    "- `row_size`: The row size of the weight matrix\n",
    "- `col_size`: The column size of the weight matrix\n",
    "- `init_val`: The initialisation values that are used to create a range of values from negative to positive\n",
    "\n",
    "and returns:\n",
    "- `init_W`: The weight matrix initialised with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cc88d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_matrix(row_size, col_size, init_val):\n",
    "    init_W = np.zeros((row_size, col_size))\n",
    "    for j in range(row_size):\n",
    "        weights = np.array(np.random.uniform(-1 * init_val, init_val, col_size)).astype(np.float32)\n",
    "        init_W[j] = weights\n",
    "    return init_W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7706b91f",
   "metadata": {},
   "source": [
    "### Network Training\n",
    "\n",
    "First we need to define the parameters of our network by initialising the weight matrix. To do this, we will create a function `network_weights` that takes inputs:\n",
    "\n",
    "- `vocab_size`: The size of the vocabulary\n",
    "- `embedding_dim`: The size of the word embeddings\n",
    "- `hidden_dim`: The list of the sizes of hidden layers. Empty if there are not hidden layers between the average embedding and the output layer\n",
    "- `num_classes`: The number of classes for the output layer\n",
    "\n",
    "and returns:\n",
    "- `W`: A dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers\n",
    "\n",
    "Note: We have to make sure that the dimensionality of each weight matrix is compatible with the previous and next weight matrix, otherwise the forward and backward pass won't be able to perform. We will also consider using `np.float32` precision to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bd04be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_weights(vocab_size=1000, embedding_dim=300,\n",
    "                   hidden_dim=[], num_classes=3, init_val=0.5):\n",
    "    \n",
    "    # initialise weights\n",
    "    W = []\n",
    "    \n",
    "    # input layer weight matrix for word embeddings\n",
    "    W.append({\n",
    "        \"embedding_matrix\": get_weight_matrix(vocab_size, embedding_dim, init_val)\n",
    "    })\n",
    "    \n",
    "    # hidden layer weight matrix\n",
    "    row_size = embedding_dim\n",
    "    for j in range(len(hidden_dim)):\n",
    "        col_size = hidden_dim[j]\n",
    "        W.append({\"weights\": get_weight_matrix(row_size, col_size, init_val)})\n",
    "        row_size = col_size\n",
    "        \n",
    "    # output layer weight matrix\n",
    "    col_size = num_classes\n",
    "    W.append({\"weights\": get_weight_matrix(row_size, col_size, init_val)})\n",
    "    \n",
    "    return W\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "184391f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'embedding_matrix': array([[ 0.19646919, -0.21386066, -0.27314854,  0.05131477],\n",
      "       [ 0.21946897, -0.07689354,  0.48076421,  0.18482974],\n",
      "       [-0.0190681 , -0.10788248, -0.15682198,  0.22904971]])}, {'weights': array([[-0.06142776, -0.4403221 ],\n",
      "       [-0.10195574,  0.2379954 ],\n",
      "       [-0.31750828, -0.32454824],\n",
      "       [ 0.03155137,  0.03182759]])}, {'weights': array([[0.13440096, 0.34943178],\n",
      "       [0.22445533, 0.11102351]])}]\n"
     ]
    }
   ],
   "source": [
    "# sanity check to see the network weight generation and their dimensions\n",
    "print(network_weights(vocab_size=3, embedding_dim=4, hidden_dim=[2], num_classes=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73099f77",
   "metadata": {},
   "source": [
    "### softmax function\n",
    "\n",
    "Due to floating point limitations in numPy, softmax frequently gets `NaN` and `inf` error. To eliminate this error, a softmax normalisation technique is used by subtracting the maximum value of z from the given z value. This way, softmax is normalised across all values for every forward pass and will not generate the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba437a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    z = z - np.max(z)\n",
    "    return np.exp(z) / np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d1b940",
   "metadata": {},
   "source": [
    "### Categorical Loss function\n",
    "\n",
    "The categorical loss function is used to calculate the objective loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdd21842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_loss(X, Y, W):\n",
    "    losses = list()\n",
    "    num_classes = 3\n",
    "    for i, x in enumerate(X):\n",
    "        out_vals = forward_pass(x, W, dropout_rate=0.0)\n",
    "        \n",
    "        # one-hot vector to represent the correct class during categorical loss calculation\n",
    "        y = one_hot_vector(Y[i]-1, num_classes)\n",
    "        loss = -y * np.log(out_vals['y_pred'].T)\n",
    "        losses.append(np.sum(loss.T))\n",
    "    return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6be798",
   "metadata": {},
   "source": [
    "### Categorical Loss Derivative function\n",
    "\n",
    "We need to also create the derivative function to calculate the derivative of categorical loss during backward pass (aka. backpropagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3760b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_loss_derivative(y, y_preds):\n",
    "    loss_der = y_preds.T - y\n",
    "    return loss_der.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b9ecd",
   "metadata": {},
   "source": [
    "### ReLU and ReLU Derivative function\n",
    "\n",
    "We will not implement `relu` function to introduce non-liniearity afte hidden layers. This function will at as activation function during forward pass.\n",
    "\n",
    "$$relu(z_i)= max(z_i,0)$$\n",
    "\n",
    "And the `relu_derivative` function is used to compute its derivative during backward pass (aka. backpropagation)\n",
    "\n",
    "  $$relu\\_derivative(z_i)=0$$ if $z_i$<=0, 1 otherwise.\n",
    "  \n",
    "Note: Both function take as input a vector $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1867c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    dz = np.copy(z)\n",
    "    dz[np.where(dz>0)] = 1\n",
    "    dz[np.where(dz<=0)] = 0\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0fbb8",
   "metadata": {},
   "source": [
    "A derivative of ReLU is differentiable at all points except at 0. The left derivative at z<0 is 0 and right derivative at z>0 is 1. But derivative at z=0 is technically undefined. In Colloquial terms, the function is not smooth at z=0 and there are many possible lines (slops) we could fit through. So that do we do here? Basically, we can arbitrarily choose a value as 0, 0.5, or 1 but for simplicity we can choose 0 for z=0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8e042",
   "metadata": {},
   "source": [
    "### Dropout mask\n",
    "\n",
    "During training, we will apply dropout mask element-wise to do model finetuning to avoid overfitting. This masking will be done after the ReLU activation function (i.e. vector of ones with random percentage set to zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1816296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(size, dropout_rate):\n",
    "    dropout_vec = np.ones((size))\n",
    "    idx = np.random.choice(len(dropout_vec), \n",
    "                          size=int(dropout_rate * size), \n",
    "                          replace=False)\n",
    "    dropout_vec[idx] = 0\n",
    "    return dropout_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aaa640cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      "[1. 0. 1. 1. 1. 1. 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(dropout_mask(10, 0.2))\n",
    "print(dropout_mask(10, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a495d1",
   "metadata": {},
   "source": [
    "### Weight submatrix\n",
    "\n",
    "Create `weight_submatrix` function that is used to generate the word embedding matrix for the given set of word indexes.\n",
    "\n",
    "This function consists of inputs\n",
    "\n",
    "- `x`: The input word index list\n",
    "- `w_emb`: the embedding weight matrix that consists of word indexes and correspoding weight values\n",
    "\n",
    "and returns\n",
    "\n",
    "- `w_sub_mat`: The newly generated word embedding submatrix for given indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b358f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_submatrix(x, w_emb):\n",
    "    w_sub_mat = np.array([])\n",
    "    \n",
    "    # Iterate through all the word indexes and lookup into weight matrix \n",
    "    # to fetch the correct embedding vector based on the index.\n",
    "    for word_inx in x:\n",
    "        \n",
    "        # find word embedding by index\n",
    "        word_w = w_emb[word_inx]\n",
    "        \n",
    "        # accummulate\n",
    "        w_sub_mat = np.append(w_sub_mat, word_w)\n",
    "        \n",
    "    w_sub_mat.shape = (int(len(w_sub_mat) / w_emb.shape[1]), w_emb.shape[1])\n",
    "    return w_sub_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784bbf5",
   "metadata": {},
   "source": [
    "### Forward pass function\n",
    "\n",
    "We are going to implement the `forward_pass` function that will pass the input through the feedforward neural network upto output layer for computing the probability of expected class using the weight matrix, ReLU activation function and softmax output function. The funtion consists of inputs\n",
    "\n",
    "- `x`: A list of vocabulary indices of the words belonging to the document\n",
    "- `W`: A list of weight matrix connecting each part of the network; e.g. W[0] is the weight matrix for connection between input and first hidden layer, W[1] is for connection between first hidden and the next next one.\n",
    "- `dropout_rate`: The dropout rate that is used to generate a random dropout mask vetor applied after each hidden layer for regularisation.\n",
    "\n",
    "and returns\n",
    "- `out_vals`: A dictionary of output value from each layer. Where layers are\n",
    "    - `h`: The vector before activation function\n",
    "    - `a`: The resulting vector after passring `h` from activation function\n",
    "    - dropout mask vector\n",
    "    - prediction vector: probability for each class from the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d545268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(x, W, dropout_rate=0.2):\n",
    "    bias = 0\n",
    "    h_vecs = []\n",
    "    a_vecs = []\n",
    "    dropout_vecs = []\n",
    "    ones_vector = np.ones((len(x), 1))\n",
    "    x_in = x\n",
    "    \n",
    "    # Iterate through hidden layers\n",
    "    for layer_idx, w in enumerate(W):\n",
    "        \n",
    "        # Compute softmax for the last layer and break the loop\n",
    "        if layer_idx == len(W) - 1:\n",
    "            y_pred = softmax(np.dot(w.get('weights').T, x_in) + bias)\n",
    "            break\n",
    "        \n",
    "        # Compute the mean embedding vector of all words for the first hidden layer\n",
    "        elif layer_idx == 0:\n",
    "\n",
    "            # Lookup into weight embeddings and create weight submatrix for a given input (list of word indexes)\n",
    "            weights = weight_submatrix(x_in, w.get('embedding_matrix'))\n",
    "\n",
    "            # Compute the mean embedding vector for first hidden layer\n",
    "            h = (np.dot(weights.T, ones_vector) / len(x_in)) + bias\n",
    "\n",
    "        # Compute the dot product between weight matrix and input for the rest of hidden layers\n",
    "        else:\n",
    "            h = np.dot(w.get('weight').T, x_in) + bias\n",
    "\n",
    "        # Apply ReLU activation function\n",
    "        a = relu(h)\n",
    "\n",
    "        # Apply dropout mask on ReLU activation function for hidden layer only\n",
    "        dropout = dropout_mask(h.shape[0], dropout_rate)\n",
    "        a = np.multiply(dropout.reshape(len(dropout), -1), a)\n",
    "        x_in = a\n",
    "\n",
    "        # Accumulate\n",
    "        h_vecs.append(h)\n",
    "        a_vecs.append(a)\n",
    "        dropout_vecs.append(dropout)\n",
    "    \n",
    "    out_vals = {}\n",
    "    out_vals['h'] = h_vecs\n",
    "    out_vals['a'] = a_vecs\n",
    "    out_vals['dropout'] = dropout_vecs\n",
    "    out_vals['y_pred'] = y_pred\n",
    "    \n",
    "    return out_vals\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f08fd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Creat one hot matrix function to generate one hot matrix for updating\n",
    "the gradients of weights during backward pass\n",
    "'''\n",
    "def one_hot_matrix(x, vocab_size):\n",
    "    ohm =np.zeros((len(x), vocab_size))\n",
    "    for i, o in enumerate(ohm):\n",
    "        o[x[i]] = 1\n",
    "    return ohm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd60303a",
   "metadata": {},
   "source": [
    "Create a `one_hot_vector` function which is used to generate the vector of true lables which will then be compared with the predicted lables to calculate categorical loss OR derivative of categorical loss. The function takes as input:\n",
    "\n",
    "- `idx`: And index of the true class\n",
    "- `size`: The vector size\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `x`: The one hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c28fa503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(idx, size):\n",
    "    x = np.zeros(size, dtype=np.int)\n",
    "    x[idx] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c3bfd",
   "metadata": {},
   "source": [
    "### Backward pass function\n",
    "\n",
    "The `backward_pass` function computes the gradients and updates the weights in the network from the output layer to the input layer. It takes inputs\n",
    "- `x`: A list of vocabulary indices each corresponding to a word in the document (input)\n",
    "- `y`: The true label\n",
    "- `W`: A list of weight matrix connecting each part of the network; e.g. W[0] is the weight matrix for connection between input and first hidden layer, W[1] is for connection between first hidden and the next next one.\n",
    "- `out_vals`: A dictionary of output values from a forward pass\n",
    "- `lr`: The learning rate that is used during weight updation\n",
    "- `freeze_emb`: A boolean value indicating whether the embedding weights will be updated or not.\n",
    "\n",
    "and returns\n",
    "- `W`: The updated weights of the network\n",
    "\n",
    "Note: The gradients on the output layer are similary to the multiclass logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26246742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(x, y, W, out_vals, lr=0.001, freeze_emb=False):\n",
    "    \n",
    "    num_classes = 3\n",
    "    \n",
    "    # One hot vector to represent the correct classes for calculating categorical loss derivative\n",
    "    y_onehot = one_hot_vector(y-1, num_classes)\n",
    "    \n",
    "    # Compute gradient on output layer (dy_hatL)\n",
    "    g = categorical_loss_derivative(y_onehot, out_vals.get(\"y_pred\"))\n",
    "    \n",
    "    for k in reversed(range(len(W))):\n",
    "        if k == len(W)-1:\n",
    "            # Compute weights gradients (ghk-1)\n",
    "            d_wkL = np.dot(out_vals.get(\"a\")[k-1], g.T)\n",
    "        else:\n",
    "            # Element-wise multiplication (g.f'(zk))\n",
    "            g = g * relu_derivative(out_vals.get(\"h\")[k])\n",
    "            if k>0:\n",
    "                # Compute weights gradients (ghk-1)\n",
    "                d_wkL = np.dot(out_vals.get(\"a\")[k-1], g.T)\n",
    "            \n",
    "            # Compute gradient only for word embedding updation\n",
    "            elif k==0 and not freeze_emb:\n",
    "                g = np.dot(np.ones((len(x), 1)), g.T) / len(x)\n",
    "        \n",
    "        if k>0:\n",
    "            # Compute the gradients w.r.t. next layer (gWk)\n",
    "            wk = W[k].get(\"weights\")\n",
    "            g = np.dot(wk, g)\n",
    "            \n",
    "            # Apply dropout for backward pass\n",
    "            dropout = out_vals[\"dropout\"][k-1]\n",
    "            g = np.multiply(dropout.reshape(len(dropout), -1), g)\n",
    "            W[k][\"weights\"] = wk - (lr * d_wkL)\n",
    "    \n",
    "    # Update word embeddings\n",
    "    if not freeze_emb:\n",
    "        d_wkL = np.dot(one_hot_matrix(x, len(news_vocab)).T, g)\n",
    "        uniqueIdx = list(set(x))\n",
    "        W[k][\"embedding_matrix\"][uniqueIdx] -= lr * d_wkL[uniqueIdx]    # Wk-lr*d_wkL\n",
    "    \n",
    "    return W\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e89463",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- Dropout mask:\n",
    "    - Dropout is used for regularisation, and it makes network less prone to overfitting. During dropout we kill the neurons during forward pass. Since the killed neurons don't contribue to the network learing process, we don't flow the gradients through them while during backward pass. Hence, it is important to apply the dropout mask just before calculating the gradient with respect to the next hidden layer (gWk) in backward pass and kill the same neuron again.\n",
    "    - The dropout mask for forward and backward pass is the same.\n",
    "\n",
    "- Word embedding updation:\n",
    "    - In order to optimise the processing time, a one hot matrix is used. This allows us to update only relevant unique indexes with resepct to the given document(x). See the logic where `W[k][\"embedding_matrix\"]` is used in `backward_pass` function.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133834e5",
   "metadata": {},
   "source": [
    "### Shuffle function\n",
    "\n",
    "We will now create a `shuffle` function that is used to shuffle the dataset before starting the training. This would ensure that the dataset is generalised during training in order to avoid the overfitting. The function take inputs:\n",
    "\n",
    "- `X`: A list of word indexes of the training dataset\n",
    "- `Y`: A list of labels of the training dataset corresponding to given X\n",
    "\n",
    "and returns:\n",
    "\n",
    "- `X_shuffled`: The shuffled list of `X`\n",
    "- `Y_shuffled`: The shuffled list of `Y` corresponding to the respective `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f5fd0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X, Y):\n",
    "    indexes = np.arange(len(X))\n",
    "    np.random.shuffle(indexes)\n",
    "    X_shuffled = X[indexes]\n",
    "    Y_shuffled = Y[indexes]\n",
    "    return X_shuffled.tolist(), Y_shuffled.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42e0af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_hot_vector(idx, size):\n",
    "    x = np.zerors(size, dtype=np.int)\n",
    "    x[idx] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e90246d",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent function\n",
    "\n",
    "We are going to create a stochastic gradient descent function `SGD` to support back-propagation by using the `forward_pass` and `backward_pass` function. It takes as inputs:\n",
    "- `X_tr`: A vector of training data\n",
    "- `Y_tr`: A vector of labels for training data\n",
    "- `W`: The weights of the network (dictionary)\n",
    "- `X_dev`: A vector of dev data\n",
    "- `Y_dev`: A vector labels for dev data\n",
    "- `lr`: Learning rate\n",
    "- `dropout`: Regularisation strength\n",
    "- `epochs`: Number of full passes over the training data\n",
    "- `tolerance`: Stop training if the difference between the current and previous vaildation loss is smaller than the threshold\n",
    "- `freeze_emb`: A boolean value indicating whether the embedding weights will be updated or not\n",
    "- `print_progress`: Flag for printing the training progress\n",
    "\n",
    "and returns:\n",
    "- `weights`: The learned weights\n",
    "- `training_loss_history`: A vector of the average losses of the entire training set after each epoch\n",
    "- `validation_loss_history`: A vector of the average losses of the entire validation set after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d5fa2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(X_tr, Y_tr, W, X_dev=[], Y_dev=[], lr=0.001, dropout=0.2, \n",
    "        epochs=5, tolerance=0.001, freeze_emb=False, print_progress=True):\n",
    "    \n",
    "    training_loss_history = []\n",
    "    validation_loss_history = []\n",
    "    \n",
    "    # Randomise the order of the training dataset\n",
    "    X_tr, Y_tr = shuffle(np.array(X_tr, dtype=object), np.array(Y_tr))\n",
    "        \n",
    "    for epoch in tnrange(epochs, desc=\"Epochs\"):\n",
    "        \n",
    "        # Iterate through the dataset\n",
    "        for i, x in enumerate(X_tr):\n",
    "            \n",
    "            # Forward pass\n",
    "            out_vals = forward_pass(x, W, dropout_rate=dropout)\n",
    "            \n",
    "            # Backward pass\n",
    "            W = backward_pass(X_tr[i], Y_tr[i], W, out_vals, lr, freeze_emb)\n",
    "            \n",
    "        # Calculate loss\n",
    "        train_loss = categorical_loss(X_tr, Y_tr, W)\n",
    "        val_loss = categorical_loss(X_dev, Y_dev, W)\n",
    "        \n",
    "        if print_progress:\n",
    "            diff = 0.0\n",
    "            if epoch > 0:\n",
    "                diff = np.abs(val_loss - validation_loss_history[-1])\n",
    "            \n",
    "            if (epoch > 0 and val_loss <0.5 and diff <= tolerance):\n",
    "                print(f'{epoch + 1}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}, Diff(Val Loss): {diff:.5f}')\n",
    "                print('Tolerance encountered. Stop the training.')\n",
    "                break\n",
    "            if (int((epoch+1)%5)) == 0:\n",
    "                print(f'{epoch + 1}, Training Loss: {train_loss:.5f}, Validation Loss: {val_loss:.5f}, Diff(Val Loss): {diff:.5f}')\n",
    "        \n",
    "        training_loss_history.append(train_loss)\n",
    "        validation_loss_history.append(val_loss)\n",
    "\n",
    "    return W, training_loss_history, validation_loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca360aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now create the function to plot the losses\n",
    "def plot_loss(loss_tr, loss_val):\n",
    "    fig, ax = plt.subplots(1, figsize=(10,5))\n",
    "    ax.plot(loss_tr, color='red', label='Training Loss')\n",
    "    ax.plot(loss_val, color='blue', label='Validation Loss')\n",
    "    ax.set_title(\"Epoch-wise loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend(fancybox=True, framealpha=1, shadow=True, borderpad=1)\n",
    "    SMALL_SIZE=12\n",
    "    MEDIUM_SIZE=14\n",
    "    BIGGER_SIZE=16\n",
    "\n",
    "    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "    plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50c69f",
   "metadata": {},
   "source": [
    "We are not ready to train and evaluate our Feedforward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03a5104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(params_list, embedding_matrix=None):\n",
    "    for i, params in enumerate(params_list):\n",
    "        \n",
    "        # Resetting seed because of randomness processing\n",
    "        set_seed()\n",
    "        \n",
    "        print(f'\\n')\n",
    "        print(f'params: {json.dumps(params, indent=4)}')\n",
    "        \n",
    "        # Define network weights\n",
    "        W = network_weights(vocab_size=params[\"vocab_size\"],\n",
    "                            embedding_dim=params[\"embedding_dim\"],\n",
    "                            hidden_dim=params[\"hidden_dim\"],\n",
    "                            num_classes=params[\"num_classes\"],\n",
    "                            init_val=params[\"init_val\"])\n",
    "        \n",
    "        # Print network weights\n",
    "        for i in range(len(W)):\n",
    "            if i==0:\n",
    "                print(f'Shape W{str(i)}, {W[i][\"embedding_matrix\"].shape}')\n",
    "            else:\n",
    "                print(f'Shape W{str(i)}, {W[i][\"weights\"].shape}')\n",
    "        \n",
    "        # Replace the weights of the embedding matrix\n",
    "        if embedding_matrix is not None:\n",
    "            W[0][\"embedding_matrix\"] = embedding_matrix\n",
    "            \n",
    "        \n",
    "        # Train the model\n",
    "        W, loss_tr, loss_val = SGD(X_tr=news_Xtrain_ngrams_indices, \n",
    "                                   Y_tr=news_ytrain, W=W,\n",
    "                                   X_dev=news_Xdev_ngrams_indices,\n",
    "                                   Y_dev=news_ydev,\n",
    "                                   lr=params[\"lr\"],\n",
    "                                   dropout=params[\"dropout\"],\n",
    "                                   freeze_emb=params[\"freeze_emb\"],\n",
    "                                   tolerance=params[\"tolerance\"],\n",
    "                                   epochs=params[\"epochs\"])\n",
    "        print(\"Learning process:\\n\")\n",
    "        plot_loss(loss_tr, loss_val)\n",
    "        \n",
    "        print(\"Compute accuracy, precision, recall, and F1-score:\\n\")\n",
    "        X_te = news_Xtest_ngrams_indices\n",
    "        Y_te = news_ytest\n",
    "        pred_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y_pred'])+1 for x,y in zip(X_te, Y_te)]\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy_score(Y_te, pred_te)}\")\n",
    "        print(f\"Precision: {precision_score(Y_te, pred_te, average='macro')}\")\n",
    "        print(f\"Recall: {recall_score(Y_te, pred_te, average='macro')}\")\n",
    "        print(f\"F1-score: {f1_score(Y_te, pred_te, average='macro')}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d3096d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "params: {\n",
      "    \"vocab_size\": 2000,\n",
      "    \"embedding_dim\": 100,\n",
      "    \"hidden_dim\": [],\n",
      "    \"lr\": 0.01,\n",
      "    \"dropout\": 0.0,\n",
      "    \"tolerance\": 0.0001,\n",
      "    \"epochs\": 200,\n",
      "    \"freeze_emb\": false,\n",
      "    \"init_val\": 0.1,\n",
      "    \"num_classes\": 3\n",
      "}\n",
      "Shape W0, (2000, 100)\n",
      "Shape W1, (100, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6bd4dd22784dde878b94f182b3f499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5, Training Loss: 1.08259, Validation Loss: 1.08921, Diff(Val Loss): 0.00340\n",
      "10, Training Loss: 0.95554, Validation Loss: 1.01122, Diff(Val Loss): 0.03028\n",
      "15, Training Loss: 0.66523, Validation Loss: 0.76548, Diff(Val Loss): 0.05261\n",
      "20, Training Loss: 0.42588, Validation Loss: 0.51929, Diff(Val Loss): 0.04160\n",
      "25, Training Loss: 0.28813, Validation Loss: 0.38814, Diff(Val Loss): 0.01832\n",
      "30, Training Loss: 0.21435, Validation Loss: 0.32857, Diff(Val Loss): 0.00874\n",
      "35, Training Loss: 0.16746, Validation Loss: 0.29907, Diff(Val Loss): 0.00445\n",
      "40, Training Loss: 0.13532, Validation Loss: 0.28395, Diff(Val Loss): 0.00226\n",
      "45, Training Loss: 0.11217, Validation Loss: 0.27627, Diff(Val Loss): 0.00116\n",
      "50, Training Loss: 0.09489, Validation Loss: 0.27241, Diff(Val Loss): 0.00056\n",
      "55, Training Loss: 0.08158, Validation Loss: 0.27043, Diff(Val Loss): 0.00034\n",
      "60, Training Loss: 0.07108, Validation Loss: 0.26962, Diff(Val Loss): 0.00009\n",
      "Tolerance encountered. Stop the training.\n",
      "Learning process:\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute accuracy, precision, recall, and F1-score:\n",
      "\n",
      "Accuracy: 0.85\n",
      "Precision: 0.8518875485406152\n",
      "Recall: 0.85\n",
      "F1-score: 0.8498986849290119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-8279170683c5>:24: RuntimeWarning: invalid value encountered in true_divide\n",
      "  h = (np.dot(weights.T, ones_vector) / len(x_in)) + bias\n"
     ]
    }
   ],
   "source": [
    "params_list = [\n",
    "    {\"vocab_size\":len(news_vocab),\n",
    "     \"embedding_dim\":100,\n",
    "     \"hidden_dim\":[],\n",
    "     \"lr\":0.01,\n",
    "     \"dropout\":0.0,\n",
    "     \"tolerance\":0.0001,\n",
    "     \"epochs\":200,\n",
    "     \"freeze_emb\":False,\n",
    "     \"init_val\":0.1,\n",
    "     \"num_classes\":3\n",
    "    }\n",
    "]\n",
    "\n",
    "train_and_evaluate(params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6809110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c1de9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
